{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d18e165",
   "metadata": {},
   "source": [
    "# 01 - Spark Architecture & Execution Model\n",
    "\n",
    "This notebook covers four foundational topics:\n",
    "\n",
    "1. **End-to-end query execution** - Job -> Stage -> Task breakdown\n",
    "2. **Narrow vs. wide transformations** - the shuffle line\n",
    "3. **Lazy evaluation** - when Spark actually does work\n",
    "4. **Client mode vs. Cluster mode** - driver placement and trade-offs\n",
    "\n",
    "Every concept is demonstrated on real data with\n",
    "a running standalone cluster and Spark UI.\n",
    "\n",
    "Sources:\n",
    "- https://spark.apache.org/docs/latest/cluster-overview.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8abba4e",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "It's assumed you already have a standalone Spark cluster running:\n",
    "- Spark UI: http://localhost:4040\n",
    "\n",
    "Dataset: transaction categorization parquet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43947c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c15e9716",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- transaction_description: string (nullable = true)\n",
      " |-- category: string (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- currency: string (nullable = true)\n",
      "\n",
      "rows: 4501043\n",
      "partitions: 4\n"
     ]
    }
   ],
   "source": [
    "# Load parquet\n",
    "txn = spark.read.parquet(r\"C:\\code\\spark-tuning-handbook\\data\\transaction_cat.parquet\")\n",
    "txn.printSchema()\n",
    "print(\"rows:\", txn.count())\n",
    "print(\"partitions:\", txn.rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d998c12",
   "metadata": {},
   "source": [
    "Spark UI: http://localhost:4040/jobs/\n",
    "You should see a few jobs from setup.\n",
    "We will use the UI to confirm:\n",
    "- job count and durations\n",
    "- number of stages\n",
    "- tasks per stage\n",
    "- shuffle read/write\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "158dac79",
   "metadata": {},
   "source": [
    "## Topic 1 - Architecture: Job -> Stage -> Task\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c1b44b2",
   "metadata": {},
   "source": [
    "### Execution: from action to tasks\n",
    "\n",
    "When you call an action (`count()`, `collect()`, `show()`, `write`, `save`, etc), Spark runs work like this:\n",
    "\n",
    "Driver -> DAGScheduler -> Stages (split by shuffle boundaries) -> Tasks (1 task per partition per stage) -> Executors (running on Worker Machines)\n",
    "\n",
    "**Rules to remember**\n",
    "\n",
    "- **1 action = 1 job** - each `count()`, `show()`, `collect()`, `write.*` submits one job  \n",
    "- **Shuffle = stage boundary** - wide transformations (`groupBy`, `join`, `repartition`, `orderBy`, `distinct`) create a new stage  \n",
    "- **1 partition = 1 task** - within a stage, Spark launches one task per Spark partition \n",
    "\n",
    "So if you have 8 partitions and 1 shuffle, you get **2 stages Ã— 8 tasks = 16 tasks**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4a0ec1",
   "metadata": {},
   "source": [
    "### Demo: 1 action -> 1 job\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "11f93221-02ba-486e-a17c-39db2c932a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.adaptive.enabled\", \"false\") # for clarity and simplicity in Spark UI\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"8\") # we don't need default 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e7f87cda-9a91-4588-9d05-4c716b5f7830",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[transaction_description: string, category: string, country: string, currency: string]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txn.unpersist(blocking=True) # clear cache, don't proceed until done (blocking=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "04d033ab-cf97-4019-8cff-b0e39fce7152",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4501043"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txn.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54792076",
   "metadata": {},
   "source": [
    "Spark UI -> Jobs -> latest job\n",
    "Expect:\n",
    "- 1 stage\n",
    "- 8 tasks (1 per partition)\n",
    "- Shuffle Write = 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb8d40ce",
   "metadata": {},
   "source": [
    "## Topic 2 - Narrow vs. Wide\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f17024",
   "metadata": {},
   "source": [
    "### Narrow transformations\n",
    "\n",
    "Narrow transformations are partition-local: they do not move data across the network.\n",
    "Examples: `filter`, `select`, `withColumn`.\n",
    "\n",
    "Multiple narrow transformations can run in one stage.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "83276e9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "899163"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# narrow -> narrow -> narrow\n",
    "narrow = (\n",
    "    txn\n",
    "    .filter(F.col(\"country\") == \"USA\")\n",
    "    .withColumn(\"desc_len\", F.length(\"transaction_description\"))\n",
    "    .select(\"country\", \"currency\", \"desc_len\")\n",
    ")\n",
    "\n",
    "narrow.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97914c7d",
   "metadata": {},
   "source": [
    "Spark UI -> Jobs -> latest job\n",
    "Expect:\n",
    "- 1 stage\n",
    "- 4 tasks\n",
    "- no shuffle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d355da",
   "metadata": {},
   "source": [
    "### Wide transformations\n",
    "\n",
    "Wide transformations require data movement (shuffle) across partitions.\n",
    "Examples: `groupBy`, `join`, `repartition`, `orderBy`, `distinct`.\n",
    "\n",
    "A shuffle creates a stage boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "27aa2cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# narrow -> narrow -> WIDE\n",
    "wide = (\n",
    "    txn\n",
    "    .filter(F.col(\"country\") == \"USA\") # produces very few rows, only some reduce partitions actually get data and only those report shuffle read, others remain empty.\n",
    "    .withColumn(\"desc_len\", F.length(\"transaction_description\"))\n",
    "    .groupBy(\"country\")\n",
    "    .agg(F.count(\"*\").alias(\"txn_count\"), F.avg(\"desc_len\").alias(\"avg_desc_len\"))\n",
    ")\n",
    "\n",
    "result = wide.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d4572c4e-9565-497b-8131-d1237968ea66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(country='AUSTRALIA', txn_count=901765, avg_desc_len=19.834742976274306)\n",
      "Row(country='INDIA', txn_count=901544, avg_desc_len=19.06047514042576)\n",
      "Row(country='UK', txn_count=899915, avg_desc_len=18.45531744664774)\n",
      "Row(country='USA', txn_count=899163, avg_desc_len=18.633907311577545)\n",
      "Row(country='CANADA', txn_count=898656, avg_desc_len=19.254804953174517)\n"
     ]
    }
   ],
   "source": [
    "print(*result, sep = '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4076f794",
   "metadata": {},
   "source": [
    "Spark UI -> Jobs -> latest job\n",
    "Expect:\n",
    "- 2 stages\n",
    "- Stage 0: partial aggregation + shuffle write\n",
    "- Stage 1: shuffle read + final aggregation\n",
    "- Shuffle Read/Write > 0\n",
    "\n",
    "Shuffle Write - data written to disk when Spark redistributes rows across partitions.\n",
    "Shuffle Read - data read from other partitions after redistribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da9fd9ce",
   "metadata": {},
   "source": [
    "### Demo: multiple shuffles -> multiple stages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "42e02263",
   "metadata": {},
   "outputs": [],
   "source": [
    "multi = (\n",
    "    txn\n",
    "    .groupBy(\"country\", \"category\").count()\n",
    "    .repartition(8, F.col(\"country\"))\n",
    ")\n",
    "\n",
    "demo = multi.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "053b6491",
   "metadata": {},
   "source": [
    "Spark UI -> Jobs -> latest job\n",
    "Expect:\n",
    "- 3 stages (2 shuffles -> 3 stages)\n",
    "- Shuffle Read/Write visible in stages twice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "bf052825-386d-4a7e-9a6f-b3c4f517a208",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(country='AUSTRALIA', category='Transportation', count=89996)\n",
      "Row(country='AUSTRALIA', category='Income', count=91072)\n",
      "Row(country='AUSTRALIA', category='Utilities & Services', count=90891)\n",
      "Row(country='AUSTRALIA', category='Shopping & Retail', count=90110)\n",
      "Row(country='AUSTRALIA', category='Financial Services', count=90377)\n"
     ]
    }
   ],
   "source": [
    "print(*demo[:5], sep = '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8898ced",
   "metadata": {},
   "source": [
    "## Topic 3 - Lazy Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b8e35ed",
   "metadata": {},
   "source": [
    "### Theory\n",
    "\n",
    "Spark transformations are **lazy** - they don't execute when you call them.\n",
    "Instead, each transformation appends a node to an internal **DAG** (Directed Acyclic Graph)\n",
    "that describes the computation.\n",
    "\n",
    "Execution happens **only** when you call an **action**:\n",
    "\n",
    "**Transformations (lazy):** `filter`, `select`, `groupBy`, `join`, `withColumn`, `orderBy`, `repartition`, ... \n",
    "**Actions (trigger execution):** `count`, `collect`, `show`, `take`, `first`, `write.*`, `foreach`, ...\n",
    "\n",
    "**Why laziness is powerful:**\n",
    "\n",
    "1. **Whole-plan optimization.** Spark can see the full computation before it touches data, so it can\n",
    "   reduce work (e.g., filter early, avoid reading unused columns, pick a better join strategy).\n",
    "\n",
    "2. **Pipelining.** Multiple narrow transformations can be fused into a single pass over each partition\n",
    "   (no intermediate shuffle / no extra stages)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e396b534",
   "metadata": {},
   "source": [
    "### Demo: transformations are instant, actions are not\n",
    "\n",
    "The cells below define a pipeline (no action yet), then trigger it with `show()`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "51177964",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 31.2 ms\n",
      "Wall time: 63.2 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[currency: string, txn_count: bigint, avg_len: double]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Define a complex pipeline - nothing executes here\n",
    "pipeline = (\n",
    "    txn\n",
    "    .filter(F.col(\"country\") == \"USA\") # narrow transformation\n",
    "    .withColumn(\"desc_upper\", F.upper(F.col(\"transaction_description\"))) # narrow transformation\n",
    "    .withColumn(\"desc_len\", F.length(F.col(\"transaction_description\"))) # narrow transformation\n",
    "    .groupBy(\"currency\") # wide transformation\n",
    "    .agg(F.count(\"*\").alias(\"txn_count\"), F.avg(\"desc_len\").alias(\"avg_len\"))\n",
    "    .orderBy(\"txn_count\", ascending=False) # wide transformation \n",
    ")\n",
    "\n",
    "# no action\n",
    "pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d37f47dd",
   "metadata": {},
   "source": [
    "Execution time should be small.\n",
    "Spark UI -> Jobs tab\n",
    "Expect: no new job yet.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "cca84f78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+------------------+\n",
      "|currency|txn_count|           avg_len|\n",
      "+--------+---------+------------------+\n",
      "|     USD|   899163|18.633907311577545|\n",
      "+--------+---------+------------------+\n",
      "\n",
      "CPU times: total: 0 ns\n",
      "Wall time: 979 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Action triggers execution\n",
    "pipeline.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c78680cb",
   "metadata": {},
   "source": [
    "Execution time will be higher.\n",
    "Spark UI -> Jobs tab\n",
    "Expect: a new job appears with multiple stages.\n",
    "\n",
    "NB: groupBy + agg shuffles once, and orderBy can use that already-partitioned output instead of reshuffling again."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e55873ab",
   "metadata": {},
   "source": [
    "## Topic 4 - Client Mode vs. Cluster Mode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d29cc4",
   "metadata": {},
   "source": [
    "### What you are running right now\n",
    "\n",
    "**Client mode**\n",
    "\n",
    "- The driver runs on your local machine (your Jupyter kernel process)\n",
    "- Executors run on the cluster worker nodes\n",
    "- Spark UI is exposed on your machine (http://localhost:4040)\n",
    "- If you stop your notebook or kill the process, the job stops (driver dies)\n",
    "\n",
    "Used for interactive work, notebooks, debugging."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93577eaa",
   "metadata": {},
   "source": [
    "**Cluster mode**\n",
    "\n",
    "- The driver runs inside the cluster (on one of the nodes)\n",
    "- Executors run on the cluster worker nodes \n",
    "- Spark UI is hosted on the driver node in the cluster\n",
    "- The job continues even if your local machine disconnects\n",
    "\n",
    "Used for production workloads and spark-submit jobs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35958562",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "**Job** - Created by each action. One `count()` / `show()` / `write` = one job.\n",
    "**Stage** - A chunk of work between shuffle boundaries. N shuffles -> N+1 stages.\n",
    "**Task** - Smallest unit of work - one task per partition per stage.\n",
    "**Narrow transformation** - Partition-local (filter, select, map). Pipelined within a stage, no shuffle.\n",
    "**Wide transformation** - Requires shuffle (groupBy, join, repartition). Creates a new stage.\n",
    "**Lazy evaluation** - Transformations build a DAG. Only actions trigger execution. Enables whole-plan optimization.\n",
    "**Client mode** - Driver on your machine. Use for notebooks / interactive work.\n",
    "**Cluster mode** - Driver on a cluster node. Use for production `spark-submit` jobs.\n",
    "\n",
    "Spark UI is the fastest way to understand what happened after an action:\n",
    "- Jobs tab -> jobs and durations\n",
    "- Stages tab -> stages per job, shuffle read/write\n",
    "- Stage detail -> task count, duration spread, skew signals"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

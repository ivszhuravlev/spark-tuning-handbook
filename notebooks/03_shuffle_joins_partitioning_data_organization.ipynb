{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ea2726f",
   "metadata": {},
   "source": [
    "# 03 - Shuffle, Joins & Partitioning\n",
    "\n",
    "This notebook combines three performance-critical Spark topics in one flow:\n",
    "\n",
    "1. **Shuffle** - what it is, why it creates stage boundaries, and why it is expensive\n",
    "2. **Joins** - Broadcast Hash Join, Sort-Merge Join, Shuffle Hash Join\n",
    "3. **Partitioning and data organization** - `repartition()` vs `coalesce()`, partitioning vs bucketing\n",
    "\n",
    "Dataset: based NYC Taxi  dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c99680d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import broadcast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f5f339e-0598-459a-ba6b-52949813bb20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "import os\n",
    "\n",
    "base_url = \"https://d37ci6vzurychx.cloudfront.net/trip-data\"\n",
    "save_dir = r\"C:\\code\\spark-tuning-handbook\\data\\taxi\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "for month in range(1, 13):\n",
    "    fname = f\"yellow_tripdata_2024-{month:02d}.parquet\"\n",
    "    url = f\"{base_url}/{fname}\"\n",
    "    dest = os.path.join(save_dir, fname)\n",
    "    if not os.path.exists(dest):\n",
    "        print(f\"Downloading {fname}...\")\n",
    "        urllib.request.urlretrieve(url, dest)\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "13ce4d6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- VendorID: integer (nullable = true)\n",
      " |-- tpep_pickup_datetime: timestamp_ntz (nullable = true)\n",
      " |-- tpep_dropoff_datetime: timestamp_ntz (nullable = true)\n",
      " |-- passenger_count: long (nullable = true)\n",
      " |-- trip_distance: double (nullable = true)\n",
      " |-- RatecodeID: long (nullable = true)\n",
      " |-- store_and_fwd_flag: string (nullable = true)\n",
      " |-- PULocationID: integer (nullable = true)\n",
      " |-- DOLocationID: integer (nullable = true)\n",
      " |-- payment_type: long (nullable = true)\n",
      " |-- fare_amount: double (nullable = true)\n",
      " |-- extra: double (nullable = true)\n",
      " |-- mta_tax: double (nullable = true)\n",
      " |-- tip_amount: double (nullable = true)\n",
      " |-- tolls_amount: double (nullable = true)\n",
      " |-- improvement_surcharge: double (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      " |-- congestion_surcharge: double (nullable = true)\n",
      " |-- Airport_fee: double (nullable = true)\n",
      "\n",
      "rows: 41169720\n",
      "partitions: 6\n",
      "columns: ['VendorID', 'tpep_pickup_datetime', 'tpep_dropoff_datetime', 'passenger_count', 'trip_distance', 'RatecodeID', 'store_and_fwd_flag', 'PULocationID', 'DOLocationID', 'payment_type', 'fare_amount', 'extra', 'mta_tax', 'tip_amount', 'tolls_amount', 'improvement_surcharge', 'total_amount', 'congestion_surcharge', 'Airport_fee']\n"
     ]
    }
   ],
   "source": [
    "# Load parquet (local path placeholder style, same as previous notebooks)\n",
    "taxi = spark.read.parquet(r\"C:\\code\\spark-tuning-handbook\\data\\taxi\")\n",
    "\n",
    "taxi.printSchema()\n",
    "print(\"rows:\", taxi.count())\n",
    "print(\"partitions:\", taxi.rdd.getNumPartitions())\n",
    "print(\"columns:\", taxi.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d068c5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark.sql.adaptive.enabled: false\n",
      "spark.sql.shuffle.partitions: 8\n",
      "spark.sql.autoBroadcastJoinThreshold: 10485760\n",
      "spark.sql.join.preferSortMergeJoin: true\n"
     ]
    }
   ],
   "source": [
    "# Keep plans deterministic for learning cells\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", \"false\")\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"8\")\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", str(10 * 1024 * 1024))  # 10 MB default\n",
    "spark.conf.set(\"spark.sql.join.preferSortMergeJoin\", \"true\")\n",
    "\n",
    "print(\"spark.sql.adaptive.enabled:\", spark.conf.get(\"spark.sql.adaptive.enabled\"))\n",
    "print(\"spark.sql.shuffle.partitions:\", spark.conf.get(\"spark.sql.shuffle.partitions\"))\n",
    "print(\"spark.sql.autoBroadcastJoinThreshold:\", spark.conf.get(\"spark.sql.autoBroadcastJoinThreshold\"))\n",
    "print(\"spark.sql.join.preferSortMergeJoin:\", spark.conf.get(\"spark.sql.join.preferSortMergeJoin\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0bafc83",
   "metadata": {},
   "source": [
    "Spark UI: http://localhost:4040/jobs/\n",
    "\n",
    "We use Spark UI in this lab after each action to validate:\n",
    "- how many stages were generated\n",
    "- where stage boundaries appear\n",
    "- shuffle write in upstream stage(s)\n",
    "- shuffle read in downstream stage(s)\n",
    "- whether one or a few tasks are much slower (skew signal)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "396e0d3b",
   "metadata": {},
   "source": [
    "## Helper - find Exchange operators quickly\n",
    "\n",
    "`explain(\"formatted\")` is the main tool in this notebook.\n",
    "The helper below extracts Exchange-related nodes from the executed plan so you can quickly confirm shuffle boundaries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bae9a125",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "def show_exchange_nodes(df):\n",
    "    plan = df._jdf.queryExecution().executedPlan().toString()\n",
    "    lines = [line.strip() for line in plan.splitlines() if \"Exchange\" in line]\n",
    "\n",
    "    if not lines:\n",
    "        print(\"No Exchange nodes in executed physical plan.\")\n",
    "    else:\n",
    "        print(\"Exchange-related nodes:\")\n",
    "        for line in lines:\n",
    "            print(line)\n",
    "\n",
    "\n",
    "def show_join_nodes(df):\n",
    "    plan = df._jdf.queryExecution().executedPlan().toString()\n",
    "    lines = [line.strip() for line in plan.splitlines() if \"Join\" in line]\n",
    "\n",
    "    if not lines:\n",
    "        print(\"No Join operator found in executed physical plan.\")\n",
    "    else:\n",
    "        print(\"Join-related nodes:\")\n",
    "        for line in lines:\n",
    "            print(line)\n",
    "\n",
    "\n",
    "# Executes an action under a unique job group ID (visible in Spark UI), then collects and prints the job and stage IDs it produced.\n",
    "def run_and_report(action_label, action_fn):\n",
    "    sc = spark.sparkContext\n",
    "    tracker = sc.statusTracker()\n",
    "    group_id = f\"notebook_demo_{int(time.time() * 1000)}\"\n",
    "\n",
    "    sc.setJobGroup(group_id, action_label)\n",
    "    try:\n",
    "        result = action_fn()\n",
    "    finally:\n",
    "        job_ids = list(tracker.getJobIdsForGroup(group_id))\n",
    "        stage_ids = set()\n",
    "        for job_id in job_ids:\n",
    "            job_info = tracker.getJobInfo(job_id)\n",
    "            if job_info is not None:\n",
    "                stage_ids.update(list(job_info.stageIds))\n",
    "\n",
    "        print(\n",
    "            f\"{action_label} -> jobs={job_ids}, stages={sorted(stage_ids)}, stage_count={len(stage_ids)}\"\n",
    "        )\n",
    "        sc.setJobGroup(\"\", \"\")\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def reset_join_defaults():\n",
    "    spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", str(10 * 1024 * 1024))\n",
    "    spark.conf.set(\"spark.sql.join.preferSortMergeJoin\", \"true\")\n",
    "    spark.conf.set(\"spark.sql.adaptive.enabled\", \"false\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "929cc4af",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 1 - Shuffle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "604fb1ad",
   "metadata": {},
   "source": [
    "### What is spill?\n",
    "\n",
    "**Spill** means Spark had to move intermediate in-memory execution data to disk because memory was insufficient for the current operator.\n",
    "\n",
    "**Where** spill commonly happens:\n",
    "- Aggregations: groupBy with many unique keys (high cardinality)\n",
    "- Joins: Sorting phase of Sort-Merge Join or building tables in Shuffle Hash Join\n",
    "- Sorting: orderBy or sort operations\n",
    "- Window Functions: Processing large partitions with OVER(...)\n",
    "- Shuffle Read: Buffering and de-serializing incoming data from other executors\n",
    "\n",
    "**Why** Spark spills to disk:\n",
    "- dropping partial state is not allowed\n",
    "- prevents OOM by using disk as a safety net\n",
    "- keeps jobs alive at the expense of heavy I/O\n",
    "\n",
    "**How** spill files are created:\n",
    "- task builds in-memory buffers: sort buffer (for sorting) or hash map (for aggregation/joins)\n",
    "- when memory thresholds are exceeded, Spark writes sort/hash chunks to local disk\n",
    "- later, Spark merges spill files to produce final output for downstream operators\n",
    "\n",
    "**Performance impact** of spill:\n",
    "- more local disk I/O\n",
    "- additional CPU for merge phases\n",
    "- —Åreates stragglers (long-tail tasks) that delay the entire stage completion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f18b84",
   "metadata": {},
   "source": [
    "### Spill demo\n",
    "Spill depends on runtime memory pressure, so exact spill bytes vary by machine.\n",
    "This experiment is deterministic in plan shape and often shows spill in Spark UI when resources are constrained:\n",
    "- reduce shuffle partitions (larger per-partition workload)\n",
    "- run wide aggregation + global sort\n",
    "- inspect stage task metrics for spill counters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c62f822e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(2) Sort [total_amount#279 ASC NULLS FIRST, tip_amount#276 ASC NULLS FIRST, trip_distance#267 ASC NULLS FIRST], true, 0\n",
      "+- Exchange rangepartitioning(total_amount#279 ASC NULLS FIRST, tip_amount#276 ASC NULLS FIRST, trip_distance#267 ASC NULLS FIRST, 1), ENSURE_REQUIREMENTS, [plan_id=916]\n",
      "   +- *(1) ColumnarToRow\n",
      "      +- FileScan parquet [VendorID#263,tpep_pickup_datetime#264,tpep_dropoff_datetime#265,passenger_count#266L,trip_distance#267,RatecodeID#268L,store_and_fwd_flag#269,PULocationID#270,DOLocationID#271,payment_type#272L,fare_amount#273,extra#274,mta_tax#275,tip_amount#276,tolls_amount#277,improvement_surcharge#278,total_amount#279,congestion_surcharge#280,Airport_fee#281] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/C:/code/spark-tuning-handbook/data/taxi], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<VendorID:int,tpep_pickup_datetime:timestamp_ntz,tpep_dropoff_datetime:timestamp_ntz,passen...\n",
      "\n",
      "\n",
      "Exchange-related nodes:\n",
      "+- Exchange rangepartitioning(total_amount#279 ASC NULLS FIRST, tip_amount#276 ASC NULLS FIRST, trip_distance#267 ASC NULLS FIRST, 1), ENSURE_REQUIREMENTS, [plan_id=916]\n"
     ]
    }
   ],
   "source": [
    "spill_test_partitions_bkp = spark.conf.get(\"spark.sql.shuffle.partitions\")\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"1\")\n",
    "\n",
    "spill_demo = (\n",
    "    taxi\n",
    "    .orderBy(\"total_amount\", \"tip_amount\", \"trip_distance\")\n",
    ")\n",
    "\n",
    "spill_demo.explain()\n",
    "show_exchange_nodes(spill_demo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3cacce7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spill experiment -> jobs=[23], stages=[39, 40], stage_count=2\n",
      "restored spark.sql.shuffle.partitions: 6\n"
     ]
    }
   ],
   "source": [
    "# Sorting 41M rows in a single partition exceeded executor's available execution memory, forcing Spark to spill 6.5 GiB to memory and 1.6 GiB to disk.\n",
    "run_and_report(\"spill experiment\", lambda: spill_demo.tail(1))\n",
    "\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", spill_test_partitions_bkp)\n",
    "print(\"restored spark.sql.shuffle.partitions:\", spark.conf.get(\"spark.sql.shuffle.partitions\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e39a06",
   "metadata": {},
   "source": [
    "**Spill verification**:\n",
    "- Spark UI -> Stages -> stage detail -> Task metrics\n",
    "- inspect `Spill (Memory)` and `Spill (Disk)`\n",
    "- if both remain zero/not displayed, increase workload volume or tighten executor memory for this experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb83fb7c",
   "metadata": {},
   "source": [
    "### Spill practical mitigation\n",
    "\n",
    "Mitigation checklist:\n",
    "- increase executor memory when possible\n",
    "- tune `spark.memory.fraction` carefully to rebalance execution vs storage (cache/persist)\n",
    "- tune `spark.sql.shuffle.partitions` so partition size is reasonable\n",
    "- pre-aggregate before expensive joins when logic allows\n",
    "- broadcast truly small side of joins to remove one redistribution path\n",
    "- fix skew (salting, AQE skew optimization, split hot keys)\n",
    "- repartition intelligently by the join/aggregation key before expensive steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e34124",
   "metadata": {},
   "source": [
    "## What shuffle is and why it is expensive\n",
    "\n",
    "A **shuffle** is the process of redistributing data across partitions so that data with the same key ends up in the same partition. This typically involves copying data across executors and machines, making the shuffle a complex and costly operation. \n",
    "\n",
    "Typical shuffle-triggering wide transformations: `groupBy` / aggregations by key, `distinct`, `repartition`, `orderBy` (global sort), joins (except broadcast), etc. Key-dependent operations that require a global data reorganization.\n",
    "\n",
    "Why shuffle creates a **stage boundary**:\n",
    "- upstream tasks (map side) must finish producing shuffle files first (buffered in memory, spill to disk if needed)\n",
    "- shuffle transfers partitioned shuffle blocks fetched from local disk across executors over the network\n",
    "- downstream tasks (reduce side) cannot start until required shuffle partitions are available\n",
    "\n",
    "=> Spark DAG scheduler splits execution into separate stages at `Exchange` (see Phisical plan)\n",
    "\n",
    "Shuffle write vs shuffle read:\n",
    "- **shuffle write** (map side): each map task writes its partitioned shuffle output to a local shuffle data file (sequential blocks)\n",
    "- **shuffle read** (reduce side): each reduce task fetches needed blocks over the network from many map tasks, then aggregation/join/sort\n",
    "\n",
    "Why **shuffle is expensive**:\n",
    "- serialization cost before writing blocks\n",
    "- disk I/O for spill/write shuffle files\n",
    "- moving data across the cluster via the network\n",
    "- deserialization cost on read\n",
    "- memory pressure while buffering/sorting/hash-building"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e470fe2",
   "metadata": {},
   "source": [
    "### Demo 1 - `groupBy()` shuffle\n",
    "\n",
    "`groupBy(\"PULocationID\")` requires all rows for each `PULocationID` to meet in the same partition for final aggregation.\n",
    "That requires key-based redistribution, so we expect an `Exchange` and a stage boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6178f938",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(2) HashAggregate(keys=[PULocationID#270], functions=[count(1), sum(total_amount#279)])\n",
      "+- Exchange hashpartitioning(PULocationID#270, 6), ENSURE_REQUIREMENTS, [plan_id=813]\n",
      "   +- *(1) HashAggregate(keys=[PULocationID#270], functions=[partial_count(1), partial_sum(total_amount#279)])\n",
      "      +- *(1) ColumnarToRow\n",
      "         +- FileScan parquet [PULocationID#270,total_amount#279] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/C:/code/spark-tuning-handbook/data/taxi], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<PULocationID:int,total_amount:double>\n",
      "\n",
      "\n",
      "Exchange-related nodes:\n",
      "+- Exchange hashpartitioning(PULocationID#270, 6), ENSURE_REQUIREMENTS, [plan_id=813]\n"
     ]
    }
   ],
   "source": [
    "grouped = (\n",
    "    taxi\n",
    "    .groupBy(\"PULocationID\")\n",
    "    .agg(F.count(\"*\").alias(\"row_count\"), F.sum(\"total_amount\").alias(\"total_sales\"))\n",
    ")\n",
    "\n",
    "grouped.explain()\n",
    "show_exchange_nodes(grouped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "cdc2e197",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---------+--------------------+\n",
      "|PULocationID|row_count|total_sales         |\n",
      "+------------+---------+--------------------+\n",
      "|186         |1362156  |3.3822911449999966E7|\n",
      "|234         |1105439  |2.472046954999828E7 |\n",
      "|263         |766745   |1.6355274009999966E7|\n",
      "|10          |15770    |1037939.5900000082  |\n",
      "|90          |652648   |1.4660598599999804E7|\n",
      "|239         |1142282  |2.47995970299976E7  |\n",
      "|4           |71461    |1687265.0800000008  |\n",
      "|209         |87461    |2612556.970000002   |\n",
      "|161         |1914607  |4.703328904000025E7 |\n",
      "|45          |65301    |1812732.3700000045  |\n",
      "+------------+---------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "groupBy aggregation action -> jobs=[24], stages=[41, 42], stage_count=2\n"
     ]
    }
   ],
   "source": [
    "run_and_report(\"groupBy aggregation action\", lambda: grouped.show(10, truncate=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20b34c9-8f9a-4234-92d5-149c17f3d7ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#in case of confusion with lambdas, it's basically the same code, same result\n",
    "\n",
    "def my_func():\n",
    "    return grouped.show(10, truncate=False)\n",
    "\n",
    "# pass fucntion as object\n",
    "run_and_report(\"action\", my_func)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe7dd5f",
   "metadata": {},
   "source": [
    "Physical-plan reading:\n",
    "- Look for `HashAggregate` (partial) -> `Exchange hashpartitioning(store_id, ...)` -> `HashAggregate` (final).\n",
    "- The `Exchange` is the shuffle boundary.\n",
    "\n",
    "Stage interpretation:\n",
    "- upstream stage writes shuffle blocks (shuffle write > 0)\n",
    "- downstream stage reads those blocks (shuffle read > 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8853179e",
   "metadata": {},
   "source": [
    "### Demo 2 - `distinct()` shuffle\n",
    "\n",
    "`distinct()` is logically a deduplication (`dropDuplicates()`) by all selected columns. To remove duplicates globally, Spark groups identical keys across partitions, which requires shuffle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d482d24a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(2) HashAggregate(keys=[PULocationID#270, DOLocationID#271], functions=[])\n",
      "+- Exchange hashpartitioning(PULocationID#270, DOLocationID#271, 6), ENSURE_REQUIREMENTS, [plan_id=984]\n",
      "   +- *(1) HashAggregate(keys=[PULocationID#270, DOLocationID#271], functions=[])\n",
      "      +- *(1) ColumnarToRow\n",
      "         +- FileScan parquet [PULocationID#270,DOLocationID#271] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/C:/code/spark-tuning-handbook/data/taxi], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<PULocationID:int,DOLocationID:int>\n",
      "\n",
      "\n",
      "Exchange-related nodes:\n",
      "+- Exchange hashpartitioning(PULocationID#270, DOLocationID#271, 6), ENSURE_REQUIREMENTS, [plan_id=984]\n"
     ]
    }
   ],
   "source": [
    "distinct_pairs = taxi.select(\"PULocationID\", \"DOLocationID\").distinct()\n",
    "\n",
    "distinct_pairs.explain()\n",
    "show_exchange_nodes(distinct_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "70337238",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distinct count action -> jobs=[25], stages=[43, 44, 45], stage_count=3\n",
      "distinct rows: 50518\n"
     ]
    }
   ],
   "source": [
    "distinct_rows = run_and_report(\"distinct count action\", lambda: distinct_pairs.count())\n",
    "\n",
    "print(\"distinct rows:\", distinct_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2cedf6b",
   "metadata": {},
   "source": [
    "Physical-plan reading:\n",
    "- Expect aggregate-style dedup operators and `Exchange hashpartitioning(...)`.\n",
    "- Dedup without data movement is not possible when duplicates can be in different partitions.\n",
    "\n",
    "Stage interpretation:\n",
    "- map side writes per-key shuffle buckets\n",
    "- reduce side reads buckets and emits unique keys\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee6e34a",
   "metadata": {},
   "source": [
    "### Demo 3 - `orderBy()` shuffle\n",
    "\n",
    "Global `orderBy()`/`sort()` requires a global ordering guarantee.\n",
    "A global order cannot be produced partition-locally, so Spark introduces range partitioning + sort work.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "3e895d1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(2) Sort [total_amount#279 DESC NULLS LAST], true, 0\n",
      "+- Exchange rangepartitioning(total_amount#279 DESC NULLS LAST, 6), ENSURE_REQUIREMENTS, [plan_id=1094]\n",
      "   +- *(1) ColumnarToRow\n",
      "      +- FileScan parquet [VendorID#263,tpep_pickup_datetime#264,tpep_dropoff_datetime#265,passenger_count#266L,trip_distance#267,RatecodeID#268L,store_and_fwd_flag#269,PULocationID#270,DOLocationID#271,payment_type#272L,fare_amount#273,extra#274,mta_tax#275,tip_amount#276,tolls_amount#277,improvement_surcharge#278,total_amount#279,congestion_surcharge#280,Airport_fee#281] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/C:/code/spark-tuning-handbook/data/taxi], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<VendorID:int,tpep_pickup_datetime:timestamp_ntz,tpep_dropoff_datetime:timestamp_ntz,passen...\n",
      "\n",
      "\n",
      "Exchange-related nodes:\n",
      "+- Exchange rangepartitioning(total_amount#279 DESC NULLS LAST, 6), ENSURE_REQUIREMENTS, [plan_id=1094]\n"
     ]
    }
   ],
   "source": [
    "ordered = taxi.orderBy(F.col(\"total_amount\").desc())\n",
    "\n",
    "ordered.explain()\n",
    "show_exchange_nodes(ordered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "3acd8cdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|total_amount|\n",
      "+------------+\n",
      "|335550.94   |\n",
      "|334145.3    |\n",
      "|50558.68    |\n",
      "|12903.4     |\n",
      "|9792.0      |\n",
      "+------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "orderBy total_amount -> jobs=[28], stages=[48], stage_count=1\n"
     ]
    }
   ],
   "source": [
    "run_and_report(\"orderBy total_amount\", lambda: ordered.select(\"total_amount\").show(5, truncate=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a42debe",
   "metadata": {},
   "source": [
    "Physical-plan reading:\n",
    "- Expect `Exchange rangepartitioning(...)` (or hash partitioning depending on planner path) and sort operators.\n",
    "- Global ordering introduces expensive wide dependency.\n",
    "\n",
    "Stage interpretation:\n",
    "- upstream stage redistributes rows by range/key\n",
    "- downstream stage performs final sort per output partition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "217372e3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 2 - Joins"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d61e480",
   "metadata": {},
   "source": [
    "### Join strategies\n",
    "\n",
    "Spark chooses physical join strategy from logical join + stats/configuration.\n",
    "Three core strategies:\n",
    "\n",
    "1. **Broadcast Hash Join (BHJ)** - small + any\n",
    "   - one side is small enough (default threshold - 10 MB) or explicitly hinted\n",
    "   - small side is collected on driver, broadcast to all executors, and built into an in-memory hash table\n",
    "   - large side is scanned and probed against that hash table, no shuffle needed\n",
    "   - trade-off: driver and executor memory usage for broadcast materialization\n",
    "\n",
    "2. **Sort-Merge Join (SMJ)** - large + large\n",
    "   - both sides are shuffled by join key\n",
    "   - both sides are sorted by join key\n",
    "   - merge phase: scan sorted sides (two pointersüëª), matching and joining keys\n",
    "   - default scalable strategy for large joins\n",
    "\n",
    "3. **Shuffle Hash Join (SHJ)** - medium + large\n",
    "   - both sides are shuffled by join key\n",
    "   - smaller side is built into a hash table per partition, **no global sort required**\n",
    "   - larger side is probed against that hash table\n",
    "   - can be faster than SMJ **when sort cost outweighs hash build**\n",
    "   - risk: per-partition hash table build can pressure executor memory\n",
    "\n",
    "**AQE note**:\n",
    "- with Adaptive Query Execution enabled, Spark can switch strategy at runtime\n",
    "- typical example: initially planned SMJ can become BHJ when runtime stats reveal a smaller side than expected\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a8d73d56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trips_fact rows (big): 41169720\n",
      "zone_stats rows (med): 50518 - yes, we could broadcast it but autoBroadcastJoinThreshold = -1 for demo purposes\n",
      "location_dim rows (small): 263\n"
     ]
    }
   ],
   "source": [
    "# Reusable DataFrames - saved to disk for clean physical plans\n",
    "base_path = r\"C:\\code\\spark-tuning-handbook\\data\\taxi\"\n",
    "\n",
    "taxi.select(\n",
    "    \"VendorID\", \"PULocationID\", \"DOLocationID\", \"payment_type\",\n",
    "    \"trip_distance\", \"fare_amount\", \"tip_amount\", \"total_amount\"\n",
    ").write.mode(\"overwrite\").parquet(f\"{base_path}\\\\trips_fact\")\n",
    "\n",
    "taxi.groupBy(\"PULocationID\", \"DOLocationID\").agg(\n",
    "    F.count(\"*\").alias(\"trip_count\"),\n",
    "    F.avg(\"total_amount\").alias(\"avg_amount\"),\n",
    "    F.avg(\"trip_distance\").alias(\"avg_distance\")\n",
    ").write.mode(\"overwrite\").parquet(f\"{base_path}\\\\zone_stats\")\n",
    "\n",
    "taxi.select(\n",
    "    \"PULocationID\", \"RatecodeID\", \"congestion_surcharge\", \"Airport_fee\"\n",
    ").dropDuplicates([\"PULocationID\"]).write.mode(\"overwrite\").parquet(f\"{base_path}\\\\location_dim\")\n",
    "\n",
    "# Read back - no lineage, clean plans\n",
    "trips_fact = spark.read.parquet(f\"{base_path}\\\\trips_fact\")\n",
    "zone_stats = spark.read.parquet(f\"{base_path}\\\\zone_stats\")\n",
    "location_dim = spark.read.parquet(f\"{base_path}\\\\location_dim\")\n",
    "\n",
    "print(\"trips_fact rows (big):\", trips_fact.count())\n",
    "print(\"zone_stats rows (med):\", zone_stats.count(), \"- yes, we could broadcast it but autoBroadcastJoinThreshold = -1 for demo purposes\")\n",
    "print(\"location_dim rows (small):\", location_dim.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13576619",
   "metadata": {},
   "source": [
    "### Demo 1 - Broadcast Hash Join\n",
    "\n",
    "We force BHJ with `broadcast()` hint on `location_dim`.\n",
    "This should produce `BroadcastHashJoin` in the physical plan.\n",
    "\n",
    "**NB**: BroadcastExchange is not a shuffle, it is a full copy of the small side sent to every executor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ce3ef986",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(2) Project [PULocationID#161, VendorID#160, DOLocationID#162, payment_type#163L, trip_distance#164, fare_amount#165, tip_amount#166, total_amount#167, RatecodeID#187L, congestion_surcharge#188, Airport_fee#189]\n",
      "+- *(2) BroadcastHashJoin [PULocationID#161], [PULocationID#186], Inner, BuildRight, false\n",
      "   :- *(2) Filter isnotnull(PULocationID#161)\n",
      "   :  +- *(2) ColumnarToRow\n",
      "   :     +- FileScan parquet [VendorID#160,PULocationID#161,DOLocationID#162,payment_type#163L,trip_distance#164,fare_amount#165,tip_amount#166,total_amount#167] Batched: true, DataFilters: [isnotnull(PULocationID#161)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/C:/code/spark-tuning-handbook/data/taxi/trips_fact], PartitionFilters: [], PushedFilters: [IsNotNull(PULocationID)], ReadSchema: struct<VendorID:int,PULocationID:int,DOLocationID:int,payment_type:bigint,trip_distance:double,fa...\n",
      "   +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [plan_id=298]\n",
      "      +- *(1) Filter isnotnull(PULocationID#186)\n",
      "         +- *(1) ColumnarToRow\n",
      "            +- FileScan parquet [PULocationID#186,RatecodeID#187L,congestion_surcharge#188,Airport_fee#189] Batched: true, DataFilters: [isnotnull(PULocationID#186)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/C:/code/spark-tuning-handbook/data/taxi/location_dim], PartitionFilters: [], PushedFilters: [IsNotNull(PULocationID)], ReadSchema: struct<PULocationID:int,RatecodeID:bigint,congestion_surcharge:double,Airport_fee:double>\n",
      "\n",
      "\n",
      "Join-related nodes:\n",
      "+- *(2) BroadcastHashJoin [PULocationID#161], [PULocationID#186], Inner, BuildRight, false\n",
      "Exchange-related nodes:\n",
      "+- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [plan_id=298]\n"
     ]
    }
   ],
   "source": [
    "reset_join_defaults()\n",
    "\n",
    "bhj = trips_fact.join(broadcast(location_dim), on=\"PULocationID\", how=\"inner\")\n",
    "\n",
    "bhj.explain()\n",
    "show_join_nodes(bhj)\n",
    "show_exchange_nodes(bhj)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "062fa5ea-08ec-46ec-985a-b7772c203973",
   "metadata": {},
   "source": [
    "#### Spark build broadcast side (location_dim) as a separate job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "353d4ec4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BHJ action -> jobs=[13, 12], stages=[18, 19, 20], stage_count=3\n",
      "BHJ rows: 41169720\n"
     ]
    }
   ],
   "source": [
    "bhj_rows = run_and_report(\"BHJ action\", lambda: bhj.count())\n",
    "print(\"BHJ rows:\", bhj_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fcb90a8",
   "metadata": {},
   "source": [
    "**Memory behavior**:\n",
    "- no shuffle for the broadcast side - small side is collected on the driver and broadcast to all executors\n",
    "- executors build a read-only in-memory hash table from the broadcast data\n",
    "- this lives in storage memory, not execution memory => **Spark cannot spill it, it either fits entirely or causes OOM**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b308b790",
   "metadata": {},
   "source": [
    "### Demo 2 - Sort-Merge Join (forced)\n",
    "\n",
    "We disable broadcast and prefer merge strategy.\n",
    "This should produce `SortMergeJoin` with shuffle + sort on both sides.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "613cad5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(5) Project [PULocationID#161, VendorID#160, fare_amount#165, tip_amount#166, RatecodeID#187L, congestion_surcharge#188]\n",
      "+- *(5) SortMergeJoin [PULocationID#161], [PULocationID#186], Inner\n",
      "   :- *(2) Sort [PULocationID#161 ASC NULLS FIRST], false, 0\n",
      "   :  +- Exchange hashpartitioning(PULocationID#161, 8), ENSURE_REQUIREMENTS, [plan_id=444]\n",
      "   :     +- *(1) Filter isnotnull(PULocationID#161)\n",
      "   :        +- *(1) ColumnarToRow\n",
      "   :           +- FileScan parquet [VendorID#160,PULocationID#161,fare_amount#165,tip_amount#166] Batched: true, DataFilters: [isnotnull(PULocationID#161)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/C:/code/spark-tuning-handbook/data/taxi/trips_fact], PartitionFilters: [], PushedFilters: [IsNotNull(PULocationID)], ReadSchema: struct<VendorID:int,PULocationID:int,fare_amount:double,tip_amount:double>\n",
      "   +- *(4) Sort [PULocationID#186 ASC NULLS FIRST], false, 0\n",
      "      +- Exchange hashpartitioning(PULocationID#186, 8), ENSURE_REQUIREMENTS, [plan_id=453]\n",
      "         +- *(3) Filter isnotnull(PULocationID#186)\n",
      "            +- *(3) ColumnarToRow\n",
      "               +- FileScan parquet [PULocationID#186,RatecodeID#187L,congestion_surcharge#188] Batched: true, DataFilters: [isnotnull(PULocationID#186)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/C:/code/spark-tuning-handbook/data/taxi/location_dim], PartitionFilters: [], PushedFilters: [IsNotNull(PULocationID)], ReadSchema: struct<PULocationID:int,RatecodeID:bigint,congestion_surcharge:double>\n",
      "\n",
      "\n",
      "Join-related nodes:\n",
      "+- *(5) SortMergeJoin [PULocationID#161], [PULocationID#186], Inner\n",
      "Exchange-related nodes:\n",
      ":  +- Exchange hashpartitioning(PULocationID#161, 8), ENSURE_REQUIREMENTS, [plan_id=444]\n",
      "+- Exchange hashpartitioning(PULocationID#186, 8), ENSURE_REQUIREMENTS, [plan_id=453]\n"
     ]
    }
   ],
   "source": [
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", \"-1\")\n",
    "spark.conf.set(\"spark.sql.join.preferSortMergeJoin\", \"true\")\n",
    "\n",
    "smj_left = trips_fact.select(\"PULocationID\", \"VendorID\", \"fare_amount\", \"tip_amount\").hint(\"merge\")\n",
    "smj_right = location_dim.select(\"PULocationID\", \"RatecodeID\", \"congestion_surcharge\").hint(\"merge\")\n",
    "\n",
    "smj = smj_left.join(smj_right, on=\"PULocationID\", how=\"inner\")\n",
    "\n",
    "smj.explain()\n",
    "show_join_nodes(smj)\n",
    "show_exchange_nodes(smj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "af98715f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SMJ action -> jobs=[14], stages=[21, 22, 23, 24], stage_count=4\n",
      "SMJ rows: 41169720\n"
     ]
    }
   ],
   "source": [
    "smj_rows = run_and_report(\"SMJ action\", lambda: smj.count())\n",
    "print(\"SMJ rows:\", smj_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac1ba2e3",
   "metadata": {},
   "source": [
    "### Demo 3 - Shuffle Hash Join (forced)\n",
    "\n",
    "We disable broadcast, disable sort-merge preference, and apply `shuffle_hash` hint.\n",
    "This should produce `ShuffledHashJoin` if planner conditions are met.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f7f62127",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(3) Project [PULocationID#161, DOLocationID#162, total_amount#167, trip_count#178L]\n",
      "+- *(3) ShuffledHashJoin [PULocationID#161, DOLocationID#162], [PULocationID#176, DOLocationID#177], Inner, BuildRight\n",
      "   :- Exchange hashpartitioning(PULocationID#161, DOLocationID#162, 8), ENSURE_REQUIREMENTS, [plan_id=636]\n",
      "   :  +- *(1) Filter (isnotnull(PULocationID#161) AND isnotnull(DOLocationID#162))\n",
      "   :     +- *(1) ColumnarToRow\n",
      "   :        +- FileScan parquet [PULocationID#161,DOLocationID#162,total_amount#167] Batched: true, DataFilters: [isnotnull(PULocationID#161), isnotnull(DOLocationID#162)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/C:/code/spark-tuning-handbook/data/taxi/trips_fact], PartitionFilters: [], PushedFilters: [IsNotNull(PULocationID), IsNotNull(DOLocationID)], ReadSchema: struct<PULocationID:int,DOLocationID:int,total_amount:double>\n",
      "   +- Exchange hashpartitioning(PULocationID#176, DOLocationID#177, 8), ENSURE_REQUIREMENTS, [plan_id=642]\n",
      "      +- *(2) Filter (isnotnull(PULocationID#176) AND isnotnull(DOLocationID#177))\n",
      "         +- *(2) ColumnarToRow\n",
      "            +- FileScan parquet [PULocationID#176,DOLocationID#177,trip_count#178L] Batched: true, DataFilters: [isnotnull(PULocationID#176), isnotnull(DOLocationID#177)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/C:/code/spark-tuning-handbook/data/taxi/zone_stats], PartitionFilters: [], PushedFilters: [IsNotNull(PULocationID), IsNotNull(DOLocationID)], ReadSchema: struct<PULocationID:int,DOLocationID:int,trip_count:bigint>\n",
      "\n",
      "\n",
      "Join-related nodes:\n",
      "+- *(3) ShuffledHashJoin [PULocationID#161, DOLocationID#162], [PULocationID#176, DOLocationID#177], Inner, BuildRight\n",
      "Exchange-related nodes:\n",
      ":- Exchange hashpartitioning(PULocationID#161, DOLocationID#162, 8), ENSURE_REQUIREMENTS, [plan_id=636]\n",
      "+- Exchange hashpartitioning(PULocationID#176, DOLocationID#177, 8), ENSURE_REQUIREMENTS, [plan_id=642]\n"
     ]
    }
   ],
   "source": [
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", \"-1\")\n",
    "spark.conf.set(\"spark.sql.join.preferSortMergeJoin\", \"false\")\n",
    "\n",
    "shj_left = trips_fact.select(\"PULocationID\", \"DOLocationID\", \"total_amount\").hint(\"shuffle_hash\")\n",
    "shj_right = zone_stats.select(\"PULocationID\", \"DOLocationID\", \"trip_count\").hint(\"shuffle_hash\")\n",
    "\n",
    "shj = shj_left.join(shj_right, on=[\"PULocationID\", \"DOLocationID\"], how=\"inner\")\n",
    "\n",
    "shj.explain()\n",
    "show_join_nodes(shj)\n",
    "show_exchange_nodes(shj)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "093867e2-0e4f-4c8d-a06e-962c99c1a75d",
   "metadata": {},
   "source": [
    "**BuildRight** means \"build the hash table from the right branch of the join tree\". The right side (zone_stats, smaller) is hashed per partition, and the left side (trips_fact, larger) streams through and probes that hash table to find matching keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "03d8b0a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SHJ action -> jobs=[16], stages=[29, 30, 31, 32], stage_count=4\n",
      "SHJ rows: 41169720\n"
     ]
    }
   ],
   "source": [
    "shj_rows = run_and_report(\"SHJ action\", lambda: shj.count())\n",
    "print(\"SHJ rows:\", shj_rows)\n",
    "\n",
    "# Restore defaults for remaining sections\n",
    "reset_join_defaults()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78eda51d",
   "metadata": {},
   "source": [
    "### Handling data skew in joins\n",
    "\n",
    "**Skew** means key distribution is highly uneven - one or few keys hold most of the rows.\n",
    "\n",
    "Why skew is **expensive**:\n",
    "- shuffle partitions are key-driven, so **hot keys** create very large partitions\n",
    "- large skewed partitions increase spill probability\n",
    "- one or few slow tasks hold back entire stage completion (long-running stragglers)\n",
    "\n",
    "**Mitigation** techniques:\n",
    "- salting\n",
    "- AQE skew join optimization (spark.sql.adaptive.skewJoin.enabled)\n",
    "- broadcast small side to avoid shuffle (and skew) entirely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a17f6178",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+\n",
      "|join_key|count   |\n",
      "+--------+--------+\n",
      "|0       |39679338|\n",
      "|140     |804618  |\n",
      "|100     |630553  |\n",
      "|80      |20310   |\n",
      "|260     |13158   |\n",
      "|40      |7817    |\n",
      "|220     |2777    |\n",
      "|160     |2612    |\n",
      "|60      |2162    |\n",
      "|180     |2086    |\n",
      "+--------+--------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Force most rows into one join key (0) to create skew.\n",
    "skew_left = (\n",
    "    trips_fact\n",
    "    .select(\"PULocationID\", \"DOLocationID\", \"total_amount\")\n",
    "    .withColumn(\"join_key\", F.when((F.col(\"PULocationID\") % 20) == 0, F.col(\"PULocationID\")).otherwise(F.lit(0)))\n",
    ")\n",
    "skew_right = (\n",
    "    trips_fact\n",
    "    .withColumn(\n",
    "        \"join_key\",\n",
    "        F.when((F.col(\"PULocationID\") % 20) == 0, F.col(\"PULocationID\")).otherwise(F.lit(0))\n",
    "    )\n",
    "    .select(\"join_key\")\n",
    "    .dropDuplicates([\"join_key\"])\n",
    ")\n",
    "\n",
    "# Save and read back for clean plans\n",
    "skew_left.write.mode(\"overwrite\").parquet(f\"{base_path}\\\\skew_left\")\n",
    "skew_right.write.mode(\"overwrite\").parquet(f\"{base_path}\\\\skew_right\")\n",
    "skew_left = spark.read.parquet(f\"{base_path}\\\\skew_left\")\n",
    "skew_right = spark.read.parquet(f\"{base_path}\\\\skew_right\")\n",
    "\n",
    "# Show skew profile: key 0 should dominate.\n",
    "skew_left.groupBy(\"join_key\").count().orderBy(F.desc(\"count\")).show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f49f6b6e",
   "metadata": {},
   "source": [
    "### Demo - skewed join without mitigation\n",
    "\n",
    "Broadcast is disabled to make shuffle cost visible.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e12654ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(5) Project [join_key#282, PULocationID#279, DOLocationID#280, total_amount#281]\n",
      "+- *(5) SortMergeJoin [join_key#282], [join_key#287], Inner\n",
      "   :- *(2) Sort [join_key#282 ASC NULLS FIRST], false, 0\n",
      "   :  +- Exchange hashpartitioning(join_key#282, 8), ENSURE_REQUIREMENTS, [plan_id=559]\n",
      "   :     +- *(1) Filter isnotnull(join_key#282)\n",
      "   :        +- *(1) ColumnarToRow\n",
      "   :           +- FileScan parquet [PULocationID#279,DOLocationID#280,total_amount#281,join_key#282] Batched: true, DataFilters: [isnotnull(join_key#282)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/C:/code/spark-tuning-handbook/data/taxi/skew_left], PartitionFilters: [], PushedFilters: [IsNotNull(join_key)], ReadSchema: struct<PULocationID:int,DOLocationID:int,total_amount:double,join_key:int>\n",
      "   +- *(4) Sort [join_key#287 ASC NULLS FIRST], false, 0\n",
      "      +- Exchange hashpartitioning(join_key#287, 8), ENSURE_REQUIREMENTS, [plan_id=568]\n",
      "         +- *(3) Filter isnotnull(join_key#287)\n",
      "            +- *(3) ColumnarToRow\n",
      "               +- FileScan parquet [join_key#287] Batched: true, DataFilters: [isnotnull(join_key#287)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/C:/code/spark-tuning-handbook/data/taxi/skew_right], PartitionFilters: [], PushedFilters: [IsNotNull(join_key)], ReadSchema: struct<join_key:int>\n",
      "\n",
      "\n",
      "Join-related nodes:\n",
      "+- *(5) SortMergeJoin [join_key#282], [join_key#287], Inner\n",
      "Exchange-related nodes:\n",
      ":  +- Exchange hashpartitioning(join_key#282, 8), ENSURE_REQUIREMENTS, [plan_id=559]\n",
      "+- Exchange hashpartitioning(join_key#287, 8), ENSURE_REQUIREMENTS, [plan_id=568]\n"
     ]
    }
   ],
   "source": [
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", \"-1\")\n",
    "spark.conf.set(\"spark.sql.join.preferSortMergeJoin\", \"true\")\n",
    "\n",
    "skew_join = skew_left.join(skew_right, on=\"join_key\", how=\"inner\")\n",
    "\n",
    "skew_join.explain()\n",
    "show_join_nodes(skew_join)\n",
    "show_exchange_nodes(skew_join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4e3cebd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skewed join action -> jobs=[19], stages=[28, 29, 30, 31], stage_count=4\n",
      "skewed join rows: 41169720\n"
     ]
    }
   ],
   "source": [
    "skew_rows = run_and_report(\"skewed join action\", lambda: skew_join.count())\n",
    "print(\"skewed join rows:\", skew_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80971ed9",
   "metadata": {},
   "source": [
    "In Spark UI, skew shows as a few **tasks with much longer duration**, disproportionate shuffle read, and possible **spill**.\n",
    "\n",
    "_In my latest run, the skewed task took 19s and read 9.1 MiB / 40M records with 792 MiB memory spill and 5.2 MiB disk spill, while the remaining 7 tasks finished in under 0.5s with negligible shuffle read - **based skew**._\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ecd0ee2",
   "metadata": {},
   "source": [
    "### Demo - salting the hot key\n",
    "\n",
    "**Salting** adds a random bucket value (0..N) to the hot key, turning one overloaded partition into N smaller ones. The right side is duplicated across all N buckets for the hot key only, so join results stay correct.\n",
    "\n",
    "**NB**: in practice skew_right is small enough to broadcast, salting is shown here as a technique for cases where both sides are **medium/large** and broadcast is not an option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b59bc12c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Join-related nodes:\n",
      "+- *(7) SortMergeJoin [join_key#282, cast(salt#323 as bigint)], [join_key#287, salt#331L], Inner\n",
      "Exchange-related nodes:\n",
      ":  +- Exchange hashpartitioning(join_key#282, cast(salt#323 as bigint), 8), ENSURE_REQUIREMENTS, [plan_id=788]\n",
      "+- Exchange hashpartitioning(join_key#287, salt#331L, 8), ENSURE_REQUIREMENTS, [plan_id=805]\n"
     ]
    }
   ],
   "source": [
    "salt_buckets = 8\n",
    "\n",
    "skew_left_salted = (\n",
    "    skew_left\n",
    "    # For hot key (join_key == 0): assign a random salt 0..7 based on hash of DOLocationID. \n",
    "    # For cold keys: salt = 0. \n",
    "    # This splits the hot key into 8 partitions instead of one.\n",
    "    .withColumn(\"salt\", F.when(F.col(\"join_key\") == 0, F.pmod(F.hash(\"DOLocationID\"), F.lit(salt_buckets))).otherwise(F.lit(0)))\n",
    ")\n",
    "\n",
    "salt_values = spark.range(0, salt_buckets).toDF(\"salt\")\n",
    "hot_right = skew_right.filter(F.col(\"join_key\") == 0).crossJoin(salt_values) # multiply record with hot key (0) by 8 salt values\n",
    "cold_right = skew_right.filter(F.col(\"join_key\") != 0).withColumn(\"salt\", F.lit(0))\n",
    "skew_right_salted = hot_right.unionByName(cold_right)\n",
    "\n",
    "salted_join = skew_left_salted.join(skew_right_salted, on=[\"join_key\", \"salt\"], how=\"inner\")\n",
    "\n",
    "#salted_join.explain() \n",
    "show_join_nodes(salted_join)\n",
    "show_exchange_nodes(salted_join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "40bcdba8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "salted join action -> jobs=[20], stages=[32, 33, 34, 35], stage_count=4\n",
      "salted join rows: 41169720\n"
     ]
    }
   ],
   "source": [
    "salted_rows = run_and_report(\"salted join action\", lambda: salted_join.count())\n",
    "print(\"salted join rows:\", salted_rows)\n",
    "\n",
    "reset_join_defaults()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd70d30",
   "metadata": {},
   "source": [
    "How this mitigation works:\n",
    "- the hot key no longer maps to exactly one huge partition\n",
    "- work is distributed across multiple salted partitions\n",
    "- this reduces single-task pressure and distributes spill evenly, but total data volume stays the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d139a717",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark.sql.adaptive.enabled: true\n",
      "spark.sql.adaptive.skewJoin.enabled: true\n"
     ]
    }
   ],
   "source": [
    "# Optional: enable AQE skew optimization and re-run skewed join action\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.sql.adaptive.skewJoin.enabled\", \"true\") # ~ automated skew detection and salting\n",
    "\n",
    "print(\"spark.sql.adaptive.enabled:\", spark.conf.get(\"spark.sql.adaptive.enabled\"))\n",
    "print(\"spark.sql.adaptive.skewJoin.enabled:\", spark.conf.get(\"spark.sql.adaptive.skewJoin.enabled\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6e6cb732-ddcf-4060-99a8-debe11656a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep deterministic non-AQE behavior for the rest of the notebook.\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", \"false\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e55ac1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 3 - Partitioning & Data Organization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a536e338",
   "metadata": {},
   "source": [
    "### Theory: `repartition()` vs `coalesce()`\n",
    "\n",
    "`repartition()`:\n",
    "- full shuffle - produces evenly distributed partitions\n",
    "- can increase or decrease partitions\n",
    "- supports key-based repartitioning (`repartition(n, key)`)\n",
    "- introduces `Exchange` and stage boundary\n",
    "\n",
    "`coalesce()`:\n",
    "- narrow transformation when reducing partitions\n",
    "- avoids full shuffle by e by merging adjacent partitions - result can be uneven\n",
    "- can only reduce partition count efficiently\n",
    "- does not trigger a shuffle, simply merges existing partitions on the same executor without data redistribution\n",
    "\n",
    "**Performance implications**:\n",
    "- use `repartition()` when you need balanced data movement or key alignment\n",
    "- use `coalesce()` for cheap partition reduction before write/output when data is already reasonably distributed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f9fc3f",
   "metadata": {},
   "source": [
    "### Demo 1 - `repartition()`\n",
    "\n",
    "This is a full redistribution by `store_id` into 12 partitions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "02588299",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rep12 partitions: 12\n",
      "== Physical Plan ==\n",
      "Exchange hashpartitioning(PULocationID#7, 12), REPARTITION_BY_NUM, [plan_id=1031]\n",
      "+- *(1) ColumnarToRow\n",
      "   +- FileScan parquet [VendorID#0,tpep_pickup_datetime#1,tpep_dropoff_datetime#2,passenger_count#3L,trip_distance#4,RatecodeID#5L,store_and_fwd_flag#6,PULocationID#7,DOLocationID#8,payment_type#9L,fare_amount#10,extra#11,mta_tax#12,tip_amount#13,tolls_amount#14,improvement_surcharge#15,total_amount#16,congestion_surcharge#17,Airport_fee#18] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/C:/code/spark-tuning-handbook/data/taxi], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<VendorID:int,tpep_pickup_datetime:timestamp_ntz,tpep_dropoff_datetime:timestamp_ntz,passen...\n",
      "\n",
      "\n",
      "Exchange-related nodes:\n",
      "Exchange hashpartitioning(PULocationID#7, 12), REPARTITION_BY_NUM, [plan_id=1031]\n"
     ]
    }
   ],
   "source": [
    "rep12 = taxi.repartition(12, \"PULocationID\")\n",
    "print(\"rep12 partitions:\", rep12.rdd.getNumPartitions())\n",
    "\n",
    "rep12.explain()\n",
    "show_exchange_nodes(rep12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "488c900a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "repartition(12) action -> jobs=[21], stages=[36, 37, 38], stage_count=3\n",
      "rep12 rows: 41169720\n"
     ]
    }
   ],
   "source": [
    "rep12_rows = run_and_report(\"repartition(12) action\",lambda: rep12.count())\n",
    "print(\"rep12 rows:\", rep12_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d263cecb",
   "metadata": {},
   "source": [
    "Plan reading:\n",
    "- Expect `Exchange hashpartitioning(PULocationID, 12)`\n",
    "- This confirms full shuffle and stage split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace6f6c6",
   "metadata": {},
   "source": [
    "### Demo 2 - `coalesce()`\n",
    "\n",
    "This reduces partitions from current scan partition count to 2 without full redistribution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bc79adca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coal2 partitions: 2\n",
      "== Physical Plan ==\n",
      "Coalesce 2\n",
      "+- *(1) ColumnarToRow\n",
      "   +- FileScan parquet [VendorID#0,tpep_pickup_datetime#1,tpep_dropoff_datetime#2,passenger_count#3L,trip_distance#4,RatecodeID#5L,store_and_fwd_flag#6,PULocationID#7,DOLocationID#8,payment_type#9L,fare_amount#10,extra#11,mta_tax#12,tip_amount#13,tolls_amount#14,improvement_surcharge#15,total_amount#16,congestion_surcharge#17,Airport_fee#18] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/C:/code/spark-tuning-handbook/data/taxi], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<VendorID:int,tpep_pickup_datetime:timestamp_ntz,tpep_dropoff_datetime:timestamp_ntz,passen...\n",
      "\n",
      "\n",
      "No Exchange nodes in executed physical plan.\n"
     ]
    }
   ],
   "source": [
    "coal2 = taxi.coalesce(2)\n",
    "print(\"coal2 partitions:\", coal2.rdd.getNumPartitions())\n",
    "\n",
    "coal2.explain()\n",
    "show_exchange_nodes(coal2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c47ada4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coalesce(2) action -> jobs=[22], stages=[39, 40], stage_count=2\n",
      "coal2 rows: 41169720\n"
     ]
    }
   ],
   "source": [
    "coal2_rows = run_and_report(\"coalesce(2) action\",lambda: coal2.count())\n",
    "print(\"coal2 rows:\", coal2_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf617ba",
   "metadata": {},
   "source": [
    "Plan reading:\n",
    "- In the common reduce-only path, `coalesce()` does not add full `Exchange` shuffle\n",
    "- coalesce() produces fewer stages than repartition() because it avoids the shuffle Exchange"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9452ddf2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Theory: Partitioning vs Bucketing\n",
    "\n",
    "**Partitioning** (directory-based):\n",
    "- rows are physically split by partition column values into directory tree (e.g. year=2024/month=06/...)\n",
    "- enables partition pruning, Spark skips entire directories when filter matches partition columns\n",
    "- improves scan I/O by reading only relevant files\n",
    "\n",
    "\n",
    "**Bucketing** pre-sorts data at write time into a fixed number of files by `hash(key) % num_buckets`. When two tables are bucketed by the same key into the same number of buckets, Spark already knows which file maps to which bucket, so it can join or aggregate **without shuffling**.\n",
    "\n",
    "**Bucketing** (hash-based fixed buckets):\n",
    "- at write time, each row is assigned to a file by hash(key) % num_buckets\n",
    "- at read time, Spark knows data is already distributed by key => can skip shuffle for joins and aggregations\n",
    "- shuffle avoidance only works when both join sides share the same bucket key and bucket count\n",
    "\n",
    "**NB**: Bucketing requires a metastore (**Hive-only optimization**), bucket metadata (key, count, sort order) is stored in the catalog. Without it, Spark reads files as plain parquet and shuffles anyway.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f964de",
   "metadata": {},
   "source": [
    "### Demo 3 - partitioned write and partition pruning\n",
    "\n",
    "Write by `tpep_pickup_datetime` year/month, then filter by one date and inspect scan plan for partition pruning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8d650435",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "year directories: ['year=2024']\n",
      "month directories under year=2024: ['month=1', 'month=10', 'month=11', 'month=12', 'month=2', 'month=3', 'month=4', 'month=5', 'month=6', 'month=7', 'month=8', 'month=9']\n",
      "month dir count: 12\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "partitioned_path = r\"C:\\code\\spark-tuning-handbook\\data\\tmp\\taxi_partitioned_by_year_month\"\n",
    "\n",
    "taxi_to_partition = taxi.select(\n",
    "    F.year(\"tpep_pickup_datetime\").alias(\"year\"),\n",
    "    F.month(\"tpep_pickup_datetime\").alias(\"month\"),\n",
    "    \"PULocationID\", \"DOLocationID\", \"total_amount\"\n",
    ").filter(F.year(\"tpep_pickup_datetime\") == 2024) # filter out random old records\n",
    "\n",
    "taxi_to_partition.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .partitionBy(\"year\", \"month\") \\\n",
    "    .parquet(partitioned_path)\n",
    "\n",
    "year_dirs = sorted([name for name in os.listdir(partitioned_path) if name.startswith(\"year=\")])\n",
    "print(\"year directories:\", year_dirs)\n",
    "last_year = os.path.join(partitioned_path, year_dirs[-1])\n",
    "month_dirs = sorted([name for name in os.listdir(last_year) if name.startswith(\"month=\")])\n",
    "print(\"month directories under\", year_dirs[-1] + \":\", month_dirs)\n",
    "print(\"month dir count:\", len(month_dirs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "657885e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(1) ColumnarToRow\n",
      "+- FileScan parquet [PULocationID#508,DOLocationID#509,total_amount#510,year#511,month#512] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/C:/code/spark-tuning-handbook/data/tmp/taxi_partitioned_by_year_..., PartitionFilters: [isnotnull(year#511), isnotnull(month#512), (year#511 = 2024), (month#512 = 6)], PushedFilters: [], ReadSchema: struct<PULocationID:int,DOLocationID:int,total_amount:double>\n",
      "\n",
      "\n",
      "contains FileScan: True\n",
      "contains PartitionFilters: True\n",
      "partition-pruning action -> jobs=[33], stages=[53, 54], stage_count=2\n",
      "rows for year=2024/month=6: 3539170\n"
     ]
    }
   ],
   "source": [
    "partitioned_df = spark.read.parquet(partitioned_path)\n",
    "\n",
    "june_df = partitioned_df.filter((F.col(\"year\") == 2024) & (F.col(\"month\") == 6))\n",
    "june_df.explain()\n",
    "#show_exchange_nodes(pruned)\n",
    "\n",
    "plan = june_df._jdf.queryExecution().executedPlan().toString() # gets Physical plan as string via Java API\n",
    "print(\"contains FileScan:\", \"FileScan\" in plan)\n",
    "print(\"contains PartitionFilters:\", \"PartitionFilters\" in plan)\n",
    "\n",
    "june_rows = run_and_report(\"partition-pruning action\",lambda: june_df.count())\n",
    "print(\"rows for year=2024/month=6:\", june_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b235035",
   "metadata": {},
   "source": [
    "`dynamic partition pruning time: 0 ms`: filter values are known at planning time (static pruning), so no runtime (dynamic) pruning is needed. Dynamic partition pruning (DPP) applies when filter values come from the result of another query, e.g. the build (AKA small) side of a join.\n",
    "\n",
    "Plan reading:\n",
    "- PartitionFilters: [year=2024, month=6] confirms Spark pruned directories at scan time\n",
    "- `number of partitions read: 1` and `size of files read: 13.7 MiB` => only June data was touched out of ~670 MB total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f06e8ef8",
   "metadata": {},
   "source": [
    "### Demo 4 - bucketing\n",
    "\n",
    "This demo creates two bucketed tables with matching bucket key and count, then checks the physical plan to see if Spark skips shuffle for the join.\n",
    "\n",
    "Again, at write time, Spark assigns rows to files by hash(key) % num_buckets => rows with the same key always land in the same file. If both tables are bucketed by the same key into the same number of buckets, Spark can join file-to-file directly => no shuffle needed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5d0756a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bucketed tables created\n"
     ]
    }
   ],
   "source": [
    "spark.conf.set(\"spark.sql.sources.bucketing.enabled\", \"true\")\n",
    "\n",
    "spark.sql(\"DROP TABLE IF EXISTS taxi_bucketed_trips\")\n",
    "spark.sql(\"DROP TABLE IF EXISTS taxi_bucketed_zones\")\n",
    "\n",
    "trips_fact.select(\"PULocationID\", \"DOLocationID\", \"total_amount\") \\\n",
    "    .write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .bucketBy(8, \"PULocationID\") \\\n",
    "    .sortBy(\"PULocationID\") \\\n",
    "    .saveAsTable(\"taxi_bucketed_trips\")\n",
    "\n",
    "zone_stats.select(\"PULocationID\", \"DOLocationID\", \"trip_count\") \\\n",
    "    .write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .bucketBy(8, \"PULocationID\") \\\n",
    "    .sortBy(\"PULocationID\") \\\n",
    "    .saveAsTable(\"taxi_bucketed_zones\")\n",
    "\n",
    "print(\"bucketed tables created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "61fe402a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------+---------------------------------------------------------------------------------+-------+\n",
      "|col_name                    |data_type                                                                        |comment|\n",
      "+----------------------------+---------------------------------------------------------------------------------+-------+\n",
      "|PULocationID                |int                                                                              |NULL   |\n",
      "|DOLocationID                |int                                                                              |NULL   |\n",
      "|total_amount                |double                                                                           |NULL   |\n",
      "|                            |                                                                                 |       |\n",
      "|# Detailed Table Information|                                                                                 |       |\n",
      "|Catalog                     |spark_catalog                                                                    |       |\n",
      "|Database                    |default                                                                          |       |\n",
      "|Table                       |taxi_bucketed_trips                                                              |       |\n",
      "|Owner                       |ivazhu01                                                                         |       |\n",
      "|Created Time                |Tue Feb 24 19:57:36 EST 2026                                                     |       |\n",
      "|Last Access                 |UNKNOWN                                                                          |       |\n",
      "|Created By                  |Spark 3.5.1                                                                      |       |\n",
      "|Type                        |MANAGED                                                                          |       |\n",
      "|Provider                    |parquet                                                                          |       |\n",
      "|Num Buckets                 |8                                                                                |       |\n",
      "|Bucket Columns              |[`PULocationID`]                                                                 |       |\n",
      "|Sort Columns                |[`PULocationID`]                                                                 |       |\n",
      "|Statistics                  |118484645 bytes                                                                  |       |\n",
      "|Location                    |file:/C:/code/spark-tuning-handbook/notebooks/spark-warehouse/taxi_bucketed_trips|       |\n",
      "|Serde Library               |org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe                      |       |\n",
      "|InputFormat                 |org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat                    |       |\n",
      "|OutputFormat                |org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat                   |       |\n",
      "+----------------------------+---------------------------------------------------------------------------------+-------+\n",
      "\n",
      "+----------------------------+---------------------------------------------------------------------------------+-------+\n",
      "|col_name                    |data_type                                                                        |comment|\n",
      "+----------------------------+---------------------------------------------------------------------------------+-------+\n",
      "|PULocationID                |int                                                                              |NULL   |\n",
      "|DOLocationID                |int                                                                              |NULL   |\n",
      "|trip_count                  |bigint                                                                           |NULL   |\n",
      "|                            |                                                                                 |       |\n",
      "|# Detailed Table Information|                                                                                 |       |\n",
      "|Catalog                     |spark_catalog                                                                    |       |\n",
      "|Database                    |default                                                                          |       |\n",
      "|Table                       |taxi_bucketed_zones                                                              |       |\n",
      "|Owner                       |ivazhu01                                                                         |       |\n",
      "|Created Time                |Tue Feb 24 19:57:37 EST 2026                                                     |       |\n",
      "|Last Access                 |UNKNOWN                                                                          |       |\n",
      "|Created By                  |Spark 3.5.1                                                                      |       |\n",
      "|Type                        |MANAGED                                                                          |       |\n",
      "|Provider                    |parquet                                                                          |       |\n",
      "|Num Buckets                 |8                                                                                |       |\n",
      "|Bucket Columns              |[`PULocationID`]                                                                 |       |\n",
      "|Sort Columns                |[`PULocationID`]                                                                 |       |\n",
      "|Statistics                  |230690 bytes                                                                     |       |\n",
      "|Location                    |file:/C:/code/spark-tuning-handbook/notebooks/spark-warehouse/taxi_bucketed_zones|       |\n",
      "|Serde Library               |org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe                      |       |\n",
      "|InputFormat                 |org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat                    |       |\n",
      "|OutputFormat                |org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat                   |       |\n",
      "+----------------------------+---------------------------------------------------------------------------------+-------+\n",
      "\n",
      "== Physical Plan ==\n",
      "*(3) Project [PULocationID#284, DOLocationID#285, total_amount#286, DOLocationID#291, trip_count#292L]\n",
      "+- *(3) SortMergeJoin [PULocationID#284], [PULocationID#290], Inner\n",
      "   :- *(1) Sort [PULocationID#284 ASC NULLS FIRST], false, 0\n",
      "   :  +- *(1) Filter isnotnull(PULocationID#284)\n",
      "   :     +- *(1) ColumnarToRow\n",
      "   :        +- FileScan parquet spark_catalog.default.taxi_bucketed_trips[PULocationID#284,DOLocationID#285,total_amount#286] Batched: true, Bucketed: true, DataFilters: [isnotnull(PULocationID#284)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/C:/code/spark-tuning-handbook/notebooks/spark-warehouse/taxi_buc..., PartitionFilters: [], PushedFilters: [IsNotNull(PULocationID)], ReadSchema: struct<PULocationID:int,DOLocationID:int,total_amount:double>, SelectedBucketsCount: 8 out of 8\n",
      "   +- *(2) Sort [PULocationID#290 ASC NULLS FIRST], false, 0\n",
      "      +- *(2) Filter isnotnull(PULocationID#290)\n",
      "         +- *(2) ColumnarToRow\n",
      "            +- FileScan parquet spark_catalog.default.taxi_bucketed_zones[PULocationID#290,DOLocationID#291,trip_count#292L] Batched: true, Bucketed: true, DataFilters: [isnotnull(PULocationID#290)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/C:/code/spark-tuning-handbook/notebooks/spark-warehouse/taxi_buc..., PartitionFilters: [], PushedFilters: [IsNotNull(PULocationID)], ReadSchema: struct<PULocationID:int,DOLocationID:int,trip_count:bigint>, SelectedBucketsCount: 8 out of 8\n",
      "\n",
      "\n",
      "Join-related nodes:\n",
      "+- *(3) SortMergeJoin [PULocationID#284], [PULocationID#290], Inner\n",
      "No Exchange nodes in executed physical plan.\n",
      "bucket-join action -> jobs=[16], stages=[23, 24], stage_count=2\n",
      "bucket join rows: 10360650221\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"DESCRIBE EXTENDED taxi_bucketed_trips\").show(200, truncate=False)\n",
    "spark.sql(\"DESCRIBE EXTENDED taxi_bucketed_zones\").show(200, truncate=False)\n",
    "\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", \"-1\") # force disable Broadcast for bucket demo\n",
    "\n",
    "bucket_join = spark.table(\"taxi_bucketed_trips\").join(\n",
    "    spark.table(\"taxi_bucketed_zones\"),\n",
    "    on=\"PULocationID\",\n",
    "    how=\"inner\"\n",
    ")\n",
    "\n",
    "bucket_join.explain()\n",
    "show_join_nodes(bucket_join)\n",
    "show_exchange_nodes(bucket_join)\n",
    "\n",
    "bucket_rows = run_and_report(\"bucket-join action\", lambda: bucket_join.count())\n",
    "print(\"bucket join rows:\", bucket_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "689240f8",
   "metadata": {},
   "source": [
    "Plan reading for bucketing:\n",
    "- Check DESCRIBE EXTENDED output for Num Buckets: 8 and Bucket Columns: [PULocationID].\n",
    "- If no Exchange hashpartitioning in the join plan - Spark leveraged bucket alignment, no shuffle needed.\n",
    "- If Exchange remains - planner did not use bucket metadata. Always verify in plan, never assume."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3723648a",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "**Shuffle**\n",
    "- `Exchange` marks redistribution and stage boundaries.\n",
    "- Shuffle cost is CPU + disk + network + memory pressure.\n",
    "- Spill is a correctness mechanism with performance cost; reduce it by memory/partition/skew tuning.\n",
    "\n",
    "**Joins**\n",
    "- BHJ: broadcast small side, no shuffle on broadcast path, memory trade-off.\n",
    "- SMJ: shuffle + sort on both sides, scalable default for large joins.\n",
    "- SHJ: shuffle + per-partition hash, avoids sort but can pressure memory.\n",
    "- Skew control is mandatory for stable join latency and spill reduction.\n",
    "\n",
    "**Partitioning and data organization**\n",
    "- `repartition()` is a full shuffle tool; `coalesce()` is a narrow reduce-partitions tool.\n",
    "- Directory partitioning helps scan pruning.\n",
    "- Bucketing (Hive) can reduce join shuffle only when key/count/sort/metadata/planner conditions align and are verified in plan.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

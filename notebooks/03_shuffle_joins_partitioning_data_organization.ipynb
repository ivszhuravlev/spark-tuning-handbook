{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ea2726f",
   "metadata": {},
   "source": [
    "# 03 - Shuffle, Joins & Partitioning\n",
    "\n",
    "This notebook combines three performance-critical Spark topics in one flow:\n",
    "\n",
    "1. **Shuffle** - what it is, why it creates stage boundaries, and why it is expensive\n",
    "2. **Joins** - Broadcast Hash Join, Sort-Merge Join, Shuffle Hash Join\n",
    "3. **Partitioning and data organization** - `repartition()` vs `coalesce()`, partitioning vs bucketing\n",
    "\n",
    "Dataset: based NYC Taxi  dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c99680d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import broadcast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f5f339e-0598-459a-ba6b-52949813bb20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "import os\n",
    "\n",
    "base_url = \"https://d37ci6vzurychx.cloudfront.net/trip-data\"\n",
    "save_dir = r\"C:\\code\\spark-tuning-handbook\\data\\taxi\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "for month in range(1, 13):\n",
    "    fname = f\"yellow_tripdata_2024-{month:02d}.parquet\"\n",
    "    url = f\"{base_url}/{fname}\"\n",
    "    dest = os.path.join(save_dir, fname)\n",
    "    if not os.path.exists(dest):\n",
    "        print(f\"Downloading {fname}...\")\n",
    "        urllib.request.urlretrieve(url, dest)\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "13ce4d6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- VendorID: integer (nullable = true)\n",
      " |-- tpep_pickup_datetime: timestamp_ntz (nullable = true)\n",
      " |-- tpep_dropoff_datetime: timestamp_ntz (nullable = true)\n",
      " |-- passenger_count: long (nullable = true)\n",
      " |-- trip_distance: double (nullable = true)\n",
      " |-- RatecodeID: long (nullable = true)\n",
      " |-- store_and_fwd_flag: string (nullable = true)\n",
      " |-- PULocationID: integer (nullable = true)\n",
      " |-- DOLocationID: integer (nullable = true)\n",
      " |-- payment_type: long (nullable = true)\n",
      " |-- fare_amount: double (nullable = true)\n",
      " |-- extra: double (nullable = true)\n",
      " |-- mta_tax: double (nullable = true)\n",
      " |-- tip_amount: double (nullable = true)\n",
      " |-- tolls_amount: double (nullable = true)\n",
      " |-- improvement_surcharge: double (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      " |-- congestion_surcharge: double (nullable = true)\n",
      " |-- Airport_fee: double (nullable = true)\n",
      "\n",
      "rows: 41169720\n",
      "partitions: 6\n",
      "columns: ['VendorID', 'tpep_pickup_datetime', 'tpep_dropoff_datetime', 'passenger_count', 'trip_distance', 'RatecodeID', 'store_and_fwd_flag', 'PULocationID', 'DOLocationID', 'payment_type', 'fare_amount', 'extra', 'mta_tax', 'tip_amount', 'tolls_amount', 'improvement_surcharge', 'total_amount', 'congestion_surcharge', 'Airport_fee']\n"
     ]
    }
   ],
   "source": [
    "# Load parquet (local path placeholder style, same as previous notebooks)\n",
    "taxi = spark.read.parquet(r\"C:\\code\\spark-tuning-handbook\\data\\taxi\")\n",
    "\n",
    "taxi.printSchema()\n",
    "print(\"rows:\", taxi.count())\n",
    "print(\"partitions:\", taxi.rdd.getNumPartitions())\n",
    "print(\"columns:\", taxi.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d068c5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark.sql.adaptive.enabled: false\n",
      "spark.sql.shuffle.partitions: 8\n",
      "spark.sql.autoBroadcastJoinThreshold: 10485760\n",
      "spark.sql.join.preferSortMergeJoin: true\n"
     ]
    }
   ],
   "source": [
    "# Keep plans deterministic for learning cells\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", \"false\")\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"8\")\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", str(10 * 1024 * 1024))  # 10 MB default\n",
    "spark.conf.set(\"spark.sql.join.preferSortMergeJoin\", \"true\")\n",
    "\n",
    "print(\"spark.sql.adaptive.enabled:\", spark.conf.get(\"spark.sql.adaptive.enabled\"))\n",
    "print(\"spark.sql.shuffle.partitions:\", spark.conf.get(\"spark.sql.shuffle.partitions\"))\n",
    "print(\"spark.sql.autoBroadcastJoinThreshold:\", spark.conf.get(\"spark.sql.autoBroadcastJoinThreshold\"))\n",
    "print(\"spark.sql.join.preferSortMergeJoin:\", spark.conf.get(\"spark.sql.join.preferSortMergeJoin\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0bafc83",
   "metadata": {},
   "source": [
    "Spark UI: http://localhost:4040/jobs/\n",
    "\n",
    "We use Spark UI in this lab after each action to validate:\n",
    "- how many stages were generated\n",
    "- where stage boundaries appear\n",
    "- shuffle write in upstream stage(s)\n",
    "- shuffle read in downstream stage(s)\n",
    "- whether one or a few tasks are much slower (skew signal)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "396e0d3b",
   "metadata": {},
   "source": [
    "## Helper - find Exchange operators quickly\n",
    "\n",
    "`explain(\"formatted\")` is the main tool in this notebook.\n",
    "The helper below extracts Exchange-related nodes from the executed plan so you can quickly confirm shuffle boundaries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bae9a125",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "def show_exchange_nodes(df):\n",
    "    plan = df._jdf.queryExecution().executedPlan().toString()\n",
    "    lines = [line.strip() for line in plan.splitlines() if \"Exchange\" in line]\n",
    "\n",
    "    if not lines:\n",
    "        print(\"No Exchange nodes in executed physical plan.\")\n",
    "    else:\n",
    "        print(\"Exchange-related nodes:\")\n",
    "        for line in lines:\n",
    "            print(line)\n",
    "\n",
    "\n",
    "def show_join_nodes(df):\n",
    "    plan = df._jdf.queryExecution().executedPlan().toString()\n",
    "    lines = [line.strip() for line in plan.splitlines() if \"Join\" in line]\n",
    "\n",
    "    if not lines:\n",
    "        print(\"No Join operator found in executed physical plan.\")\n",
    "    else:\n",
    "        print(\"Join-related nodes:\")\n",
    "        for line in lines:\n",
    "            print(line)\n",
    "\n",
    "\n",
    "# Executes an action under a unique job group ID (visible in Spark UI), then collects and prints the job and stage IDs it produced.\n",
    "def run_and_report(action_label, action_fn):\n",
    "    sc = spark.sparkContext\n",
    "    tracker = sc.statusTracker()\n",
    "    group_id = f\"notebook_demo_{int(time.time() * 1000)}\"\n",
    "\n",
    "    sc.setJobGroup(group_id, action_label)\n",
    "    try:\n",
    "        result = action_fn()\n",
    "    finally:\n",
    "        job_ids = list(tracker.getJobIdsForGroup(group_id))\n",
    "        stage_ids = set()\n",
    "        for job_id in job_ids:\n",
    "            job_info = tracker.getJobInfo(job_id)\n",
    "            if job_info is not None:\n",
    "                stage_ids.update(list(job_info.stageIds))\n",
    "\n",
    "        print(\n",
    "            f\"{action_label} -> jobs={job_ids}, stages={sorted(stage_ids)}, stage_count={len(stage_ids)}\"\n",
    "        )\n",
    "        sc.setJobGroup(\"\", \"\")\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def reset_join_defaults():\n",
    "    spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", str(10 * 1024 * 1024))\n",
    "    spark.conf.set(\"spark.sql.join.preferSortMergeJoin\", \"true\")\n",
    "    spark.conf.set(\"spark.sql.adaptive.enabled\", \"false\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "929cc4af",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 1 - Shuffle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "604fb1ad",
   "metadata": {},
   "source": [
    "### What is spill?\n",
    "\n",
    "**Spill** means Spark had to move intermediate in-memory execution data to disk because memory was insufficient for the current operator.\n",
    "\n",
    "**Where** spill commonly happens:\n",
    "- Aggregations: groupBy with many unique keys (high cardinality)\n",
    "- Joins: Sorting phase of Sort-Merge Join or building tables in Shuffle Hash Join\n",
    "- Sorting: orderBy or sort operations\n",
    "- Window Functions: Processing large partitions with OVER(...)\n",
    "- Shuffle Read: Buffering and de-serializing incoming data from other executors\n",
    "\n",
    "**Why** Spark spills to disk:\n",
    "- dropping partial state is not allowed\n",
    "- prevents OOM by using disk as a safety net\n",
    "- keeps jobs alive at the expense of heavy I/O\n",
    "\n",
    "**How** spill files are created:\n",
    "- task builds in-memory buffers: sort buffer (for sorting) or hash map (for aggregation/joins)\n",
    "- when memory thresholds are exceeded, Spark writes sort/hash chunks to local disk\n",
    "- later, Spark merges spill files to produce final output for downstream operators\n",
    "\n",
    "**Performance impact** of spill:\n",
    "- more local disk I/O\n",
    "- additional CPU for merge phases\n",
    "- —Åreates stragglers (long-tail tasks) that delay the entire stage completion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f18b84",
   "metadata": {},
   "source": [
    "### Spill demo\n",
    "Spill depends on runtime memory pressure, so exact spill bytes vary by machine.\n",
    "This experiment is deterministic in plan shape and often shows spill in Spark UI when resources are constrained:\n",
    "- reduce shuffle partitions (larger per-partition workload)\n",
    "- run wide aggregation + global sort\n",
    "- inspect stage task metrics for spill counters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c62f822e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(2) Sort [total_amount#279 ASC NULLS FIRST, tip_amount#276 ASC NULLS FIRST, trip_distance#267 ASC NULLS FIRST], true, 0\n",
      "+- Exchange rangepartitioning(total_amount#279 ASC NULLS FIRST, tip_amount#276 ASC NULLS FIRST, trip_distance#267 ASC NULLS FIRST, 1), ENSURE_REQUIREMENTS, [plan_id=916]\n",
      "   +- *(1) ColumnarToRow\n",
      "      +- FileScan parquet [VendorID#263,tpep_pickup_datetime#264,tpep_dropoff_datetime#265,passenger_count#266L,trip_distance#267,RatecodeID#268L,store_and_fwd_flag#269,PULocationID#270,DOLocationID#271,payment_type#272L,fare_amount#273,extra#274,mta_tax#275,tip_amount#276,tolls_amount#277,improvement_surcharge#278,total_amount#279,congestion_surcharge#280,Airport_fee#281] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/C:/code/spark-tuning-handbook/data/taxi], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<VendorID:int,tpep_pickup_datetime:timestamp_ntz,tpep_dropoff_datetime:timestamp_ntz,passen...\n",
      "\n",
      "\n",
      "Exchange-related nodes:\n",
      "+- Exchange rangepartitioning(total_amount#279 ASC NULLS FIRST, tip_amount#276 ASC NULLS FIRST, trip_distance#267 ASC NULLS FIRST, 1), ENSURE_REQUIREMENTS, [plan_id=916]\n"
     ]
    }
   ],
   "source": [
    "spill_test_partitions_bkp = spark.conf.get(\"spark.sql.shuffle.partitions\")\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"1\")\n",
    "\n",
    "spill_demo = (\n",
    "    taxi\n",
    "    .orderBy(\"total_amount\", \"tip_amount\", \"trip_distance\")\n",
    ")\n",
    "\n",
    "spill_demo.explain()\n",
    "show_exchange_nodes(spill_demo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3cacce7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spill experiment -> jobs=[23], stages=[39, 40], stage_count=2\n",
      "restored spark.sql.shuffle.partitions: 6\n"
     ]
    }
   ],
   "source": [
    "# Sorting 41M rows in a single partition exceeded executor's available execution memory, forcing Spark to spill 6.5 GiB to memory and 1.6 GiB to disk.\n",
    "run_and_report(\"spill experiment\", lambda: spill_demo.tail(1))\n",
    "\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", spill_test_partitions_bkp)\n",
    "print(\"restored spark.sql.shuffle.partitions:\", spark.conf.get(\"spark.sql.shuffle.partitions\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e39a06",
   "metadata": {},
   "source": [
    "**Spill verification**:\n",
    "- Spark UI -> Stages -> stage detail -> Task metrics\n",
    "- inspect `Spill (Memory)` and `Spill (Disk)`\n",
    "- if both remain zero/not displayed, increase workload volume or tighten executor memory for this experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb83fb7c",
   "metadata": {},
   "source": [
    "### Spill practical mitigation\n",
    "\n",
    "Mitigation checklist:\n",
    "- increase executor memory when possible\n",
    "- tune `spark.memory.fraction` carefully to rebalance execution vs storage (cache/persist)\n",
    "- tune `spark.sql.shuffle.partitions` so partition size is reasonable\n",
    "- pre-aggregate before expensive joins when logic allows\n",
    "- broadcast truly small side of joins to remove one redistribution path\n",
    "- fix skew (salting, AQE skew optimization, split hot keys)\n",
    "- repartition intelligently by the join/aggregation key before expensive steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e34124",
   "metadata": {},
   "source": [
    "## What shuffle is and why it is expensive\n",
    "\n",
    "A **shuffle** is the process of redistributing data across partitions so that data with the same key ends up in the same partition. This typically involves copying data across executors and machines, making the shuffle a complex and costly operation. \n",
    "\n",
    "Typical shuffle-triggering wide transformations: `groupBy` / aggregations by key, `distinct`, `repartition`, `orderBy` (global sort), joins (except broadcast), etc. Key-dependent operations that require a global data reorganization.\n",
    "\n",
    "Why shuffle creates a **stage boundary**:\n",
    "- upstream tasks (map side) must finish producing shuffle files first (buffered in memory, spill to disk if needed)\n",
    "- shuffle transfers partitioned shuffle blocks fetched from local disk across executors over the network\n",
    "- downstream tasks (reduce side) cannot start until required shuffle partitions are available\n",
    "\n",
    "=> Spark DAG scheduler splits execution into separate stages at `Exchange` (see Phisical plan)\n",
    "\n",
    "Shuffle write vs shuffle read:\n",
    "- **shuffle write** (map side): each map task writes its partitioned shuffle output to a local shuffle data file (sequential blocks)\n",
    "- **shuffle read** (reduce side): each reduce task fetches needed blocks over the network from many map tasks, then aggregation/join/sort\n",
    "\n",
    "Why **shuffle is expensive**:\n",
    "- serialization cost before writing blocks\n",
    "- disk I/O for spill/write shuffle files\n",
    "- moving data across the cluster via the network\n",
    "- deserialization cost on read\n",
    "- memory pressure while buffering/sorting/hash-building"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e470fe2",
   "metadata": {},
   "source": [
    "### Demo 1 - `groupBy()` shuffle\n",
    "\n",
    "`groupBy(\"PULocationID\")` requires all rows for each `PULocationID` to meet in the same partition for final aggregation.\n",
    "That requires key-based redistribution, so we expect an `Exchange` and a stage boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6178f938",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(2) HashAggregate(keys=[PULocationID#270], functions=[count(1), sum(total_amount#279)])\n",
      "+- Exchange hashpartitioning(PULocationID#270, 6), ENSURE_REQUIREMENTS, [plan_id=813]\n",
      "   +- *(1) HashAggregate(keys=[PULocationID#270], functions=[partial_count(1), partial_sum(total_amount#279)])\n",
      "      +- *(1) ColumnarToRow\n",
      "         +- FileScan parquet [PULocationID#270,total_amount#279] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/C:/code/spark-tuning-handbook/data/taxi], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<PULocationID:int,total_amount:double>\n",
      "\n",
      "\n",
      "Exchange-related nodes:\n",
      "+- Exchange hashpartitioning(PULocationID#270, 6), ENSURE_REQUIREMENTS, [plan_id=813]\n"
     ]
    }
   ],
   "source": [
    "grouped = (\n",
    "    taxi\n",
    "    .groupBy(\"PULocationID\")\n",
    "    .agg(F.count(\"*\").alias(\"row_count\"), F.sum(\"total_amount\").alias(\"total_sales\"))\n",
    ")\n",
    "\n",
    "grouped.explain()\n",
    "show_exchange_nodes(grouped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "cdc2e197",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---------+--------------------+\n",
      "|PULocationID|row_count|total_sales         |\n",
      "+------------+---------+--------------------+\n",
      "|186         |1362156  |3.3822911449999966E7|\n",
      "|234         |1105439  |2.472046954999828E7 |\n",
      "|263         |766745   |1.6355274009999966E7|\n",
      "|10          |15770    |1037939.5900000082  |\n",
      "|90          |652648   |1.4660598599999804E7|\n",
      "|239         |1142282  |2.47995970299976E7  |\n",
      "|4           |71461    |1687265.0800000008  |\n",
      "|209         |87461    |2612556.970000002   |\n",
      "|161         |1914607  |4.703328904000025E7 |\n",
      "|45          |65301    |1812732.3700000045  |\n",
      "+------------+---------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "groupBy aggregation action -> jobs=[24], stages=[41, 42], stage_count=2\n"
     ]
    }
   ],
   "source": [
    "run_and_report(\"groupBy aggregation action\", lambda: grouped.show(10, truncate=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20b34c9-8f9a-4234-92d5-149c17f3d7ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#in case of confusion with lambdas, it's basically the same code, same result\n",
    "\n",
    "def my_func():\n",
    "    return grouped.show(10, truncate=False)\n",
    "\n",
    "# pass fucntion as object\n",
    "run_and_report(\"action\", my_func)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe7dd5f",
   "metadata": {},
   "source": [
    "Physical-plan reading:\n",
    "- Look for `HashAggregate` (partial) -> `Exchange hashpartitioning(store_id, ...)` -> `HashAggregate` (final).\n",
    "- The `Exchange` is the shuffle boundary.\n",
    "\n",
    "Stage interpretation:\n",
    "- upstream stage writes shuffle blocks (shuffle write > 0)\n",
    "- downstream stage reads those blocks (shuffle read > 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8853179e",
   "metadata": {},
   "source": [
    "### Demo 2 - `distinct()` shuffle\n",
    "\n",
    "`distinct()` is logically a deduplication (`dropDuplicates()`) by all selected columns. To remove duplicates globally, Spark groups identical keys across partitions, which requires shuffle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d482d24a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(2) HashAggregate(keys=[PULocationID#270, DOLocationID#271], functions=[])\n",
      "+- Exchange hashpartitioning(PULocationID#270, DOLocationID#271, 6), ENSURE_REQUIREMENTS, [plan_id=984]\n",
      "   +- *(1) HashAggregate(keys=[PULocationID#270, DOLocationID#271], functions=[])\n",
      "      +- *(1) ColumnarToRow\n",
      "         +- FileScan parquet [PULocationID#270,DOLocationID#271] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/C:/code/spark-tuning-handbook/data/taxi], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<PULocationID:int,DOLocationID:int>\n",
      "\n",
      "\n",
      "Exchange-related nodes:\n",
      "+- Exchange hashpartitioning(PULocationID#270, DOLocationID#271, 6), ENSURE_REQUIREMENTS, [plan_id=984]\n"
     ]
    }
   ],
   "source": [
    "distinct_pairs = taxi.select(\"PULocationID\", \"DOLocationID\").distinct()\n",
    "\n",
    "distinct_pairs.explain()\n",
    "show_exchange_nodes(distinct_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "70337238",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distinct count action -> jobs=[25], stages=[43, 44, 45], stage_count=3\n",
      "distinct rows: 50518\n"
     ]
    }
   ],
   "source": [
    "distinct_rows = run_and_report(\"distinct count action\", lambda: distinct_pairs.count())\n",
    "\n",
    "print(\"distinct rows:\", distinct_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2cedf6b",
   "metadata": {},
   "source": [
    "Physical-plan reading:\n",
    "- Expect aggregate-style dedup operators and `Exchange hashpartitioning(...)`.\n",
    "- Dedup without data movement is not possible when duplicates can be in different partitions.\n",
    "\n",
    "Stage interpretation:\n",
    "- map side writes per-key shuffle buckets\n",
    "- reduce side reads buckets and emits unique keys\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee6e34a",
   "metadata": {},
   "source": [
    "### Demo 3 - `orderBy()` shuffle\n",
    "\n",
    "Global `orderBy()`/`sort()` requires a global ordering guarantee.\n",
    "A global order cannot be produced partition-locally, so Spark introduces range partitioning + sort work.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "3e895d1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(2) Sort [total_amount#279 DESC NULLS LAST], true, 0\n",
      "+- Exchange rangepartitioning(total_amount#279 DESC NULLS LAST, 6), ENSURE_REQUIREMENTS, [plan_id=1094]\n",
      "   +- *(1) ColumnarToRow\n",
      "      +- FileScan parquet [VendorID#263,tpep_pickup_datetime#264,tpep_dropoff_datetime#265,passenger_count#266L,trip_distance#267,RatecodeID#268L,store_and_fwd_flag#269,PULocationID#270,DOLocationID#271,payment_type#272L,fare_amount#273,extra#274,mta_tax#275,tip_amount#276,tolls_amount#277,improvement_surcharge#278,total_amount#279,congestion_surcharge#280,Airport_fee#281] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/C:/code/spark-tuning-handbook/data/taxi], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<VendorID:int,tpep_pickup_datetime:timestamp_ntz,tpep_dropoff_datetime:timestamp_ntz,passen...\n",
      "\n",
      "\n",
      "Exchange-related nodes:\n",
      "+- Exchange rangepartitioning(total_amount#279 DESC NULLS LAST, 6), ENSURE_REQUIREMENTS, [plan_id=1094]\n"
     ]
    }
   ],
   "source": [
    "ordered = taxi.orderBy(F.col(\"total_amount\").desc())\n",
    "\n",
    "ordered.explain()\n",
    "show_exchange_nodes(ordered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "3acd8cdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|total_amount|\n",
      "+------------+\n",
      "|335550.94   |\n",
      "|334145.3    |\n",
      "|50558.68    |\n",
      "|12903.4     |\n",
      "|9792.0      |\n",
      "+------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "orderBy total_amount -> jobs=[28], stages=[48], stage_count=1\n"
     ]
    }
   ],
   "source": [
    "run_and_report(\"orderBy total_amount\", lambda: ordered.select(\"total_amount\").show(5, truncate=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a42debe",
   "metadata": {},
   "source": [
    "Physical-plan reading:\n",
    "- Expect `Exchange rangepartitioning(...)` (or hash partitioning depending on planner path) and sort operators.\n",
    "- Global ordering introduces expensive wide dependency.\n",
    "\n",
    "Stage interpretation:\n",
    "- upstream stage redistributes rows by range/key\n",
    "- downstream stage performs final sort per output partition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "217372e3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 2 - Joins"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d61e480",
   "metadata": {},
   "source": [
    "### Join strategies\n",
    "\n",
    "Spark chooses physical join strategy from logical join + stats/configuration.\n",
    "Three core strategies:\n",
    "\n",
    "1. **Broadcast Hash Join (BHJ)** - small + any\n",
    "   - one side is small enough (default threshold - 10 MB) or explicitly hinted\n",
    "   - small side is collected on driver, broadcast to all executors, and built into an in-memory hash table\n",
    "   - large side is scanned and probed against that hash table, no shuffle needed\n",
    "   - trade-off: driver and executor memory usage for broadcast materialization\n",
    "\n",
    "2. **Sort-Merge Join (SMJ)** - large + large\n",
    "   - both sides are shuffled by join key\n",
    "   - both sides are sorted by join key\n",
    "   - merge phase: scan sorted sides (two pointersüëª), matching and joining keys\n",
    "   - default scalable strategy for large joins\n",
    "\n",
    "3. **Shuffle Hash Join (SHJ)** - medium + large\n",
    "   - both sides are shuffled by join key\n",
    "   - smaller side is built into a hash table per partition, **no global sort required**\n",
    "   - larger side is probed against that hash table\n",
    "   - can be faster than SMJ **when sort cost outweighs hash build**\n",
    "   - risk: per-partition hash table build can pressure executor memory\n",
    "\n",
    "**AQE note**:\n",
    "- with Adaptive Query Execution enabled, Spark can switch strategy at runtime\n",
    "- typical example: initially planned SMJ can become BHJ when runtime stats reveal a smaller side than expected\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a8d73d56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trips_fact rows (big): 41169720\n",
      "zone_stats rows (med): 50518 - yes, we could broadcast it but autoBroadcastJoinThreshold = -1 for demo purposes\n",
      "location_dim rows (small): 263\n"
     ]
    }
   ],
   "source": [
    "# Reusable DataFrames - saved to disk for clean physical plans\n",
    "base_path = r\"C:\\code\\spark-tuning-handbook\\data\\taxi\"\n",
    "\n",
    "taxi.select(\n",
    "    \"VendorID\", \"PULocationID\", \"DOLocationID\", \"payment_type\",\n",
    "    \"trip_distance\", \"fare_amount\", \"tip_amount\", \"total_amount\"\n",
    ").write.mode(\"overwrite\").parquet(f\"{base_path}\\\\trips_fact\")\n",
    "\n",
    "taxi.groupBy(\"PULocationID\", \"DOLocationID\").agg(\n",
    "    F.count(\"*\").alias(\"trip_count\"),\n",
    "    F.avg(\"total_amount\").alias(\"avg_amount\"),\n",
    "    F.avg(\"trip_distance\").alias(\"avg_distance\")\n",
    ").write.mode(\"overwrite\").parquet(f\"{base_path}\\\\zone_stats\")\n",
    "\n",
    "taxi.select(\n",
    "    \"PULocationID\", \"RatecodeID\", \"congestion_surcharge\", \"Airport_fee\"\n",
    ").dropDuplicates([\"PULocationID\"]).write.mode(\"overwrite\").parquet(f\"{base_path}\\\\location_dim\")\n",
    "\n",
    "# Read back - no lineage, clean plans\n",
    "trips_fact = spark.read.parquet(f\"{base_path}\\\\trips_fact\")\n",
    "zone_stats = spark.read.parquet(f\"{base_path}\\\\zone_stats\")\n",
    "location_dim = spark.read.parquet(f\"{base_path}\\\\location_dim\")\n",
    "\n",
    "print(\"trips_fact rows (big):\", trips_fact.count())\n",
    "print(\"zone_stats rows (med):\", zone_stats.count(), \"- yes, we could broadcast it but autoBroadcastJoinThreshold = -1 for demo purposes\")\n",
    "print(\"location_dim rows (small):\", location_dim.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13576619",
   "metadata": {},
   "source": [
    "### Demo 1 - Broadcast Hash Join\n",
    "\n",
    "We force BHJ with `broadcast()` hint on `location_dim`.\n",
    "This should produce `BroadcastHashJoin` in the physical plan.\n",
    "\n",
    "**NB**: BroadcastExchange is not a shuffle, it is a full copy of the small side sent to every executor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ce3ef986",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(2) Project [PULocationID#161, VendorID#160, DOLocationID#162, payment_type#163L, trip_distance#164, fare_amount#165, tip_amount#166, total_amount#167, RatecodeID#187L, congestion_surcharge#188, Airport_fee#189]\n",
      "+- *(2) BroadcastHashJoin [PULocationID#161], [PULocationID#186], Inner, BuildRight, false\n",
      "   :- *(2) Filter isnotnull(PULocationID#161)\n",
      "   :  +- *(2) ColumnarToRow\n",
      "   :     +- FileScan parquet [VendorID#160,PULocationID#161,DOLocationID#162,payment_type#163L,trip_distance#164,fare_amount#165,tip_amount#166,total_amount#167] Batched: true, DataFilters: [isnotnull(PULocationID#161)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/C:/code/spark-tuning-handbook/data/taxi/trips_fact], PartitionFilters: [], PushedFilters: [IsNotNull(PULocationID)], ReadSchema: struct<VendorID:int,PULocationID:int,DOLocationID:int,payment_type:bigint,trip_distance:double,fa...\n",
      "   +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [plan_id=298]\n",
      "      +- *(1) Filter isnotnull(PULocationID#186)\n",
      "         +- *(1) ColumnarToRow\n",
      "            +- FileScan parquet [PULocationID#186,RatecodeID#187L,congestion_surcharge#188,Airport_fee#189] Batched: true, DataFilters: [isnotnull(PULocationID#186)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/C:/code/spark-tuning-handbook/data/taxi/location_dim], PartitionFilters: [], PushedFilters: [IsNotNull(PULocationID)], ReadSchema: struct<PULocationID:int,RatecodeID:bigint,congestion_surcharge:double,Airport_fee:double>\n",
      "\n",
      "\n",
      "Join-related nodes:\n",
      "+- *(2) BroadcastHashJoin [PULocationID#161], [PULocationID#186], Inner, BuildRight, false\n",
      "Exchange-related nodes:\n",
      "+- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [plan_id=298]\n"
     ]
    }
   ],
   "source": [
    "reset_join_defaults()\n",
    "\n",
    "bhj = trips_fact.join(broadcast(location_dim), on=\"PULocationID\", how=\"inner\")\n",
    "\n",
    "bhj.explain()\n",
    "show_join_nodes(bhj)\n",
    "show_exchange_nodes(bhj)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "062fa5ea-08ec-46ec-985a-b7772c203973",
   "metadata": {},
   "source": [
    "#### Spark build broadcast side (location_dim) as a separate job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "353d4ec4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BHJ action -> jobs=[13, 12], stages=[18, 19, 20], stage_count=3\n",
      "BHJ rows: 41169720\n"
     ]
    }
   ],
   "source": [
    "bhj_rows = run_and_report(\"BHJ action\", lambda: bhj.count())\n",
    "print(\"BHJ rows:\", bhj_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fcb90a8",
   "metadata": {},
   "source": [
    "**Memory behavior**:\n",
    "- no shuffle for the broadcast side - small side is collected on the driver and broadcast to all executors\n",
    "- executors build a read-only in-memory hash table from the broadcast data\n",
    "- this lives in storage memory, not execution memory => **Spark cannot spill it, it either fits entirely or causes OOM**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b308b790",
   "metadata": {},
   "source": [
    "### Demo 2 - Sort-Merge Join (forced)\n",
    "\n",
    "We disable broadcast and prefer merge strategy.\n",
    "This should produce `SortMergeJoin` with shuffle + sort on both sides.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "613cad5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(5) Project [PULocationID#161, VendorID#160, fare_amount#165, tip_amount#166, RatecodeID#187L, congestion_surcharge#188]\n",
      "+- *(5) SortMergeJoin [PULocationID#161], [PULocationID#186], Inner\n",
      "   :- *(2) Sort [PULocationID#161 ASC NULLS FIRST], false, 0\n",
      "   :  +- Exchange hashpartitioning(PULocationID#161, 8), ENSURE_REQUIREMENTS, [plan_id=444]\n",
      "   :     +- *(1) Filter isnotnull(PULocationID#161)\n",
      "   :        +- *(1) ColumnarToRow\n",
      "   :           +- FileScan parquet [VendorID#160,PULocationID#161,fare_amount#165,tip_amount#166] Batched: true, DataFilters: [isnotnull(PULocationID#161)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/C:/code/spark-tuning-handbook/data/taxi/trips_fact], PartitionFilters: [], PushedFilters: [IsNotNull(PULocationID)], ReadSchema: struct<VendorID:int,PULocationID:int,fare_amount:double,tip_amount:double>\n",
      "   +- *(4) Sort [PULocationID#186 ASC NULLS FIRST], false, 0\n",
      "      +- Exchange hashpartitioning(PULocationID#186, 8), ENSURE_REQUIREMENTS, [plan_id=453]\n",
      "         +- *(3) Filter isnotnull(PULocationID#186)\n",
      "            +- *(3) ColumnarToRow\n",
      "               +- FileScan parquet [PULocationID#186,RatecodeID#187L,congestion_surcharge#188] Batched: true, DataFilters: [isnotnull(PULocationID#186)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/C:/code/spark-tuning-handbook/data/taxi/location_dim], PartitionFilters: [], PushedFilters: [IsNotNull(PULocationID)], ReadSchema: struct<PULocationID:int,RatecodeID:bigint,congestion_surcharge:double>\n",
      "\n",
      "\n",
      "Join-related nodes:\n",
      "+- *(5) SortMergeJoin [PULocationID#161], [PULocationID#186], Inner\n",
      "Exchange-related nodes:\n",
      ":  +- Exchange hashpartitioning(PULocationID#161, 8), ENSURE_REQUIREMENTS, [plan_id=444]\n",
      "+- Exchange hashpartitioning(PULocationID#186, 8), ENSURE_REQUIREMENTS, [plan_id=453]\n"
     ]
    }
   ],
   "source": [
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", \"-1\")\n",
    "spark.conf.set(\"spark.sql.join.preferSortMergeJoin\", \"true\")\n",
    "\n",
    "smj_left = trips_fact.select(\"PULocationID\", \"VendorID\", \"fare_amount\", \"tip_amount\").hint(\"merge\")\n",
    "smj_right = location_dim.select(\"PULocationID\", \"RatecodeID\", \"congestion_surcharge\").hint(\"merge\")\n",
    "\n",
    "smj = smj_left.join(smj_right, on=\"PULocationID\", how=\"inner\")\n",
    "\n",
    "smj.explain()\n",
    "show_join_nodes(smj)\n",
    "show_exchange_nodes(smj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "af98715f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SMJ action -> jobs=[14], stages=[21, 22, 23, 24], stage_count=4\n",
      "SMJ rows: 41169720\n"
     ]
    }
   ],
   "source": [
    "smj_rows = run_and_report(\"SMJ action\", lambda: smj.count())\n",
    "print(\"SMJ rows:\", smj_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac1ba2e3",
   "metadata": {},
   "source": [
    "### Demo 3 - Shuffle Hash Join (forced)\n",
    "\n",
    "We disable broadcast, disable sort-merge preference, and apply `shuffle_hash` hint.\n",
    "This should produce `ShuffledHashJoin` if planner conditions are met.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f7f62127",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(3) Project [PULocationID#161, DOLocationID#162, total_amount#167, trip_count#178L]\n",
      "+- *(3) ShuffledHashJoin [PULocationID#161, DOLocationID#162], [PULocationID#176, DOLocationID#177], Inner, BuildRight\n",
      "   :- Exchange hashpartitioning(PULocationID#161, DOLocationID#162, 8), ENSURE_REQUIREMENTS, [plan_id=636]\n",
      "   :  +- *(1) Filter (isnotnull(PULocationID#161) AND isnotnull(DOLocationID#162))\n",
      "   :     +- *(1) ColumnarToRow\n",
      "   :        +- FileScan parquet [PULocationID#161,DOLocationID#162,total_amount#167] Batched: true, DataFilters: [isnotnull(PULocationID#161), isnotnull(DOLocationID#162)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/C:/code/spark-tuning-handbook/data/taxi/trips_fact], PartitionFilters: [], PushedFilters: [IsNotNull(PULocationID), IsNotNull(DOLocationID)], ReadSchema: struct<PULocationID:int,DOLocationID:int,total_amount:double>\n",
      "   +- Exchange hashpartitioning(PULocationID#176, DOLocationID#177, 8), ENSURE_REQUIREMENTS, [plan_id=642]\n",
      "      +- *(2) Filter (isnotnull(PULocationID#176) AND isnotnull(DOLocationID#177))\n",
      "         +- *(2) ColumnarToRow\n",
      "            +- FileScan parquet [PULocationID#176,DOLocationID#177,trip_count#178L] Batched: true, DataFilters: [isnotnull(PULocationID#176), isnotnull(DOLocationID#177)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/C:/code/spark-tuning-handbook/data/taxi/zone_stats], PartitionFilters: [], PushedFilters: [IsNotNull(PULocationID), IsNotNull(DOLocationID)], ReadSchema: struct<PULocationID:int,DOLocationID:int,trip_count:bigint>\n",
      "\n",
      "\n",
      "Join-related nodes:\n",
      "+- *(3) ShuffledHashJoin [PULocationID#161, DOLocationID#162], [PULocationID#176, DOLocationID#177], Inner, BuildRight\n",
      "Exchange-related nodes:\n",
      ":- Exchange hashpartitioning(PULocationID#161, DOLocationID#162, 8), ENSURE_REQUIREMENTS, [plan_id=636]\n",
      "+- Exchange hashpartitioning(PULocationID#176, DOLocationID#177, 8), ENSURE_REQUIREMENTS, [plan_id=642]\n"
     ]
    }
   ],
   "source": [
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", \"-1\")\n",
    "spark.conf.set(\"spark.sql.join.preferSortMergeJoin\", \"false\")\n",
    "\n",
    "shj_left = trips_fact.select(\"PULocationID\", \"DOLocationID\", \"total_amount\").hint(\"shuffle_hash\")\n",
    "shj_right = zone_stats.select(\"PULocationID\", \"DOLocationID\", \"trip_count\").hint(\"shuffle_hash\")\n",
    "\n",
    "shj = shj_left.join(shj_right, on=[\"PULocationID\", \"DOLocationID\"], how=\"inner\")\n",
    "\n",
    "shj.explain()\n",
    "show_join_nodes(shj)\n",
    "show_exchange_nodes(shj)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "093867e2-0e4f-4c8d-a06e-962c99c1a75d",
   "metadata": {},
   "source": [
    "**BuildRight** means \"build the hash table from the right branch of the join tree\". The right side (zone_stats, smaller) is hashed per partition, and the left side (trips_fact, larger) streams through and probes that hash table to find matching keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "03d8b0a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SHJ action -> jobs=[16], stages=[29, 30, 31, 32], stage_count=4\n",
      "SHJ rows: 41169720\n"
     ]
    }
   ],
   "source": [
    "shj_rows = run_and_report(\"SHJ action\", lambda: shj.count())\n",
    "print(\"SHJ rows:\", shj_rows)\n",
    "\n",
    "# Restore defaults for remaining sections\n",
    "reset_join_defaults()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78eda51d",
   "metadata": {},
   "source": [
    "### Handling data skew in joins\n",
    "\n",
    "**Skew** means key distribution is highly uneven - one or few keys hold most of the rows.\n",
    "\n",
    "Why skew is **expensive**:\n",
    "- shuffle partitions are key-driven, so **hot keys** create very large partitions\n",
    "- large skewed partitions increase spill probability\n",
    "- one or few slow tasks hold back entire stage completion (long-running stragglers)\n",
    "\n",
    "**Mitigation** techniques:\n",
    "- salting\n",
    "- AQE skew join optimization (spark.sql.adaptive.skewJoin.enabled)\n",
    "- broadcast small side to avoid shuffle (and skew) entirely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a17f6178",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+\n",
      "|join_key|count   |\n",
      "+--------+--------+\n",
      "|0       |39679338|\n",
      "|140     |804618  |\n",
      "|100     |630553  |\n",
      "|80      |20310   |\n",
      "|260     |13158   |\n",
      "|40      |7817    |\n",
      "|220     |2777    |\n",
      "|160     |2612    |\n",
      "|60      |2162    |\n",
      "|180     |2086    |\n",
      "+--------+--------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Force most rows into one join key (0) to create skew.\n",
    "skew_left = (\n",
    "    trips_fact\n",
    "    .select(\"PULocationID\", \"DOLocationID\", \"total_amount\")\n",
    "    .withColumn(\"join_key\", F.when((F.col(\"PULocationID\") % 20) == 0, F.col(\"PULocationID\")).otherwise(F.lit(0)))\n",
    ")\n",
    "skew_right = (\n",
    "    trips_fact\n",
    "    .withColumn(\n",
    "        \"join_key\",\n",
    "        F.when((F.col(\"PULocationID\") % 20) == 0, F.col(\"PULocationID\")).otherwise(F.lit(0))\n",
    "    )\n",
    "    .select(\"join_key\")\n",
    "    .dropDuplicates([\"join_key\"])\n",
    ")\n",
    "\n",
    "# Save and read back for clean plans\n",
    "skew_left.write.mode(\"overwrite\").parquet(f\"{base_path}\\\\skew_left\")\n",
    "skew_right.write.mode(\"overwrite\").parquet(f\"{base_path}\\\\skew_right\")\n",
    "skew_left = spark.read.parquet(f\"{base_path}\\\\skew_left\")\n",
    "skew_right = spark.read.parquet(f\"{base_path}\\\\skew_right\")\n",
    "\n",
    "# Show skew profile: key 0 should dominate.\n",
    "skew_left.groupBy(\"join_key\").count().orderBy(F.desc(\"count\")).show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f49f6b6e",
   "metadata": {},
   "source": [
    "### Demo - skewed join without mitigation\n",
    "\n",
    "Broadcast is disabled to make shuffle cost visible.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e12654ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(5) Project [join_key#510, PULocationID#507, DOLocationID#508, total_amount#509]\n",
      "+- *(5) SortMergeJoin [join_key#510], [join_key#515], Inner\n",
      "   :- *(2) Sort [join_key#510 ASC NULLS FIRST], false, 0\n",
      "   :  +- Exchange hashpartitioning(join_key#510, 8), ENSURE_REQUIREMENTS, [plan_id=2081]\n",
      "   :     +- *(1) Filter isnotnull(join_key#510)\n",
      "   :        +- *(1) ColumnarToRow\n",
      "   :           +- FileScan parquet [PULocationID#507,DOLocationID#508,total_amount#509,join_key#510] Batched: true, DataFilters: [isnotnull(join_key#510)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/C:/code/spark-tuning-handbook/data/taxi/skew_left], PartitionFilters: [], PushedFilters: [IsNotNull(join_key)], ReadSchema: struct<PULocationID:int,DOLocationID:int,total_amount:double,join_key:int>\n",
      "   +- *(4) Sort [join_key#515 ASC NULLS FIRST], false, 0\n",
      "      +- Exchange hashpartitioning(join_key#515, 8), ENSURE_REQUIREMENTS, [plan_id=2090]\n",
      "         +- *(3) Filter isnotnull(join_key#515)\n",
      "            +- *(3) ColumnarToRow\n",
      "               +- FileScan parquet [join_key#515] Batched: true, DataFilters: [isnotnull(join_key#515)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/C:/code/spark-tuning-handbook/data/taxi/skew_right], PartitionFilters: [], PushedFilters: [IsNotNull(join_key)], ReadSchema: struct<join_key:int>\n",
      "\n",
      "\n",
      "Join-related nodes:\n",
      "+- *(5) SortMergeJoin [join_key#510], [join_key#515], Inner\n",
      "Exchange-related nodes:\n",
      ":  +- Exchange hashpartitioning(join_key#510, 8), ENSURE_REQUIREMENTS, [plan_id=2081]\n",
      "+- Exchange hashpartitioning(join_key#515, 8), ENSURE_REQUIREMENTS, [plan_id=2090]\n"
     ]
    }
   ],
   "source": [
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", \"-1\")\n",
    "spark.conf.set(\"spark.sql.join.preferSortMergeJoin\", \"true\")\n",
    "\n",
    "skew_join = skew_left.join(skew_right, on=\"join_key\", how=\"inner\")\n",
    "\n",
    "skew_join.explain()\n",
    "show_join_nodes(skew_join)\n",
    "show_exchange_nodes(skew_join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4e3cebd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skewed join action -> jobs=[39, 38], stages=[73, 74, 75], stage_count=3\n",
      "skewed join rows: 41169720\n"
     ]
    }
   ],
   "source": [
    "skew_rows = run_and_report(\"skewed join action\", lambda: skew_join.count())\n",
    "print(\"skewed join rows:\", skew_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80971ed9",
   "metadata": {},
   "source": [
    "In Spark UI, skew shows as a few **tasks with much longer duration**, disproportionate shuffle read, and possible **spill**.\n",
    "\n",
    "_In my latest run, the skewed task took 19s and read 9.1 MiB / 40M records with 792 MiB memory spill and 5.2 MiB disk spill, while the remaining 7 tasks finished in under 0.5s with negligible shuffle read - **based skew**._\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ecd0ee2",
   "metadata": {},
   "source": [
    "### Demo - salting the hot key\n",
    "\n",
    "**Salting** adds a random bucket value (0..N) to the hot key, turning one overloaded partition into N smaller ones. The right side is duplicated across all N buckets for the hot key only, so join results stay correct.\n",
    "\n",
    "**NB**: in practice skew_right is small enough to broadcast, salting is shown here as a technique for cases where both sides are **medium/large** and broadcast is not an option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b59bc12c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Join-related nodes:\n",
      "+- BroadcastHashJoin [join_key#510, cast(salt#586 as bigint)], [join_key#515, salt#594L], Inner, BuildRight, false\n",
      ":- BroadcastNestedLoopJoin BuildRight, Cross\n",
      "Exchange-related nodes:\n",
      "+- BroadcastExchange HashedRelationBroadcastMode(List(input[0, int, true], input[1, bigint, false]),false), [plan_id=2597]\n",
      ":  +- BroadcastExchange IdentityBroadcastMode, [plan_id=2593]\n"
     ]
    }
   ],
   "source": [
    "salt_buckets = 8\n",
    "\n",
    "skew_left_salted = (\n",
    "    skew_left\n",
    "    # For hot key (join_key == 0): assign a random salt 0..7 based on hash of DOLocationID. \n",
    "    # For cold keys: salt = 0. \n",
    "    # This splits the hot key into 8 partitions instead of one.\n",
    "    .withColumn(\"salt\", F.when(F.col(\"join_key\") == 0, F.pmod(F.hash(\"DOLocationID\"), F.lit(salt_buckets))).otherwise(F.lit(0)))\n",
    ")\n",
    "\n",
    "salt_values = spark.range(0, salt_buckets).toDF(\"salt\")\n",
    "hot_right = skew_right.filter(F.col(\"join_key\") == 0).crossJoin(salt_values) # multiply record with hot key (0) by 8 salt values\n",
    "cold_right = skew_right.filter(F.col(\"join_key\") != 0).withColumn(\"salt\", F.lit(0))\n",
    "skew_right_salted = hot_right.unionByName(cold_right)\n",
    "\n",
    "salted_join = skew_left_salted.join(skew_right_salted, on=[\"join_key\", \"salt\"], how=\"inner\")\n",
    "\n",
    "#salted_join.explain() \n",
    "show_join_nodes(salted_join)\n",
    "show_exchange_nodes(salted_join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "40bcdba8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "salted join action -> jobs=[35, 34, 33, 32], stages=[65, 66, 67, 68, 69], stage_count=5\n",
      "salted join rows: 41169720\n"
     ]
    }
   ],
   "source": [
    "salted_rows = run_and_report(\"salted join action\", lambda: salted_join.count())\n",
    "print(\"salted join rows:\", salted_rows)\n",
    "\n",
    "reset_join_defaults()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd70d30",
   "metadata": {},
   "source": [
    "How this mitigation works:\n",
    "- the hot key no longer maps to exactly one huge partition\n",
    "- work is distributed across multiple salted partitions\n",
    "- this reduces single-task pressure and distributes spill evenly, but total data volume stays the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d139a717",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark.sql.adaptive.enabled: true\n",
      "spark.sql.adaptive.skewJoin.enabled: true\n"
     ]
    }
   ],
   "source": [
    "# Optional: enable AQE skew optimization and re-run skewed join action\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.sql.adaptive.skewJoin.enabled\", \"true\") # ~ automated skew detection and salting\n",
    "\n",
    "print(\"spark.sql.adaptive.enabled:\", spark.conf.get(\"spark.sql.adaptive.enabled\"))\n",
    "print(\"spark.sql.adaptive.skewJoin.enabled:\", spark.conf.get(\"spark.sql.adaptive.skewJoin.enabled\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6e6cb732-ddcf-4060-99a8-debe11656a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep deterministic non-AQE behavior for the rest of the notebook.\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", \"false\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e55ac1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 3 - Partitioning & Data Organization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a536e338",
   "metadata": {},
   "source": [
    "### Theory: `repartition()` vs `coalesce()`\n",
    "\n",
    "`repartition()`:\n",
    "- full shuffle\n",
    "- can increase or decrease partitions\n",
    "- supports key-based repartitioning (`repartition(n, key)`)\n",
    "- introduces `Exchange` and stage boundary\n",
    "\n",
    "`coalesce()`:\n",
    "- narrow transformation when reducing partitions\n",
    "- avoids full shuffle by collapsing existing partitions\n",
    "- can only reduce partition count efficiently\n",
    "- does not introduce full `Exchange` by itself in the common reduce-only case\n",
    "\n",
    "Performance implications:\n",
    "- use `repartition()` when you need balanced data movement or key alignment\n",
    "- use `coalesce()` for cheap partition reduction before write/output when data is already reasonably distributed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f9fc3f",
   "metadata": {},
   "source": [
    "### Demo 1 - `repartition()`\n",
    "\n",
    "This is a full redistribution by `store_id` into 12 partitions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02588299",
   "metadata": {},
   "outputs": [],
   "source": [
    "rep12 = retail.repartition(12, \"store_id\")\n",
    "print(\"rep12 partitions:\", rep12.rdd.getNumPartitions())\n",
    "\n",
    "rep12.explain(\"formatted\")\n",
    "show_exchange_nodes(rep12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "488c900a",
   "metadata": {},
   "outputs": [],
   "source": [
    "rep12_rows = run_and_report(\n",
    "    \"repartition(12) action\",\n",
    "    lambda: rep12.count()\n",
    ")\n",
    "print(\"rep12 rows:\", rep12_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d263cecb",
   "metadata": {},
   "source": [
    "Plan reading:\n",
    "- Expect `Exchange hashpartitioning(store_id, 12)`.\n",
    "- This confirms full shuffle and stage split.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace6f6c6",
   "metadata": {},
   "source": [
    "### Demo 2 - `coalesce()`\n",
    "\n",
    "This reduces partitions from current scan partition count to 2 without full redistribution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc79adca",
   "metadata": {},
   "outputs": [],
   "source": [
    "coal2 = retail.coalesce(2)\n",
    "print(\"coal2 partitions:\", coal2.rdd.getNumPartitions())\n",
    "\n",
    "coal2.explain(\"formatted\")\n",
    "show_exchange_nodes(coal2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47ada4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "coal2_rows = run_and_report(\n",
    "    \"coalesce(2) action\",\n",
    "    lambda: coal2.count()\n",
    ")\n",
    "print(\"coal2 rows:\", coal2_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf617ba",
   "metadata": {},
   "source": [
    "Plan reading:\n",
    "- In the common reduce-only path, `coalesce()` does not add full `Exchange` shuffle.\n",
    "- Stage layout is narrower than `repartition()` for equivalent partition reduction intent.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9452ddf2",
   "metadata": {},
   "source": [
    "### Theory: Partitioning vs Bucketing\n",
    "\n",
    "Partitioning (directory-based):\n",
    "- rows are physically split by partition column values into directory tree (for example `dt=2024-06-30/...`)\n",
    "- very effective for predicate-driven pruning when filters hit partition columns\n",
    "- improves scan I/O by skipping unrelated directories/files\n",
    "\n",
    "Bucketing (hash-based fixed buckets):\n",
    "- rows are assigned to a fixed number of buckets by hash(key)\n",
    "- bucket metadata can help join/aggregation planning\n",
    "- shuffle avoidance is possible only when both sides are bucketed compatibly\n",
    "\n",
    "Conditions for bucket-based shuffle avoidance in joins:\n",
    "- same bucket key\n",
    "- compatible bucket count\n",
    "- compatible sort/order expectations where required by strategy\n",
    "- planner support and relevant Spark settings/table metadata must align\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f964de",
   "metadata": {},
   "source": [
    "### Demo 3 - partitioned write and partition pruning\n",
    "\n",
    "Write by `dt`, then filter by one date and inspect scan plan for partition pruning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d650435",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "partitioned_path = r\"C:\\code\\spark-tuning-handbook\\data\\tmp\\retail_partitioned_by_dt\"\n",
    "\n",
    "(\n",
    "    retail\n",
    "    .select(\"dt\", \"store_id\", \"product_id\", \"sale_amount\")\n",
    "    .write\n",
    "    .mode(\"overwrite\")\n",
    "    .partitionBy(\"dt\")\n",
    "    .parquet(partitioned_path)\n",
    ")\n",
    "\n",
    "part_dirs = sorted([name for name in os.listdir(partitioned_path) if name.startswith(\"dt=\")])\n",
    "print(\"example partition directories:\", part_dirs[:5])\n",
    "print(\"partition dir count:\", len(part_dirs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "657885e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "partitioned_df = spark.read.parquet(partitioned_path)\n",
    "\n",
    "pruned = partitioned_df.filter(F.col(\"dt\") == \"2024-06-30\")\n",
    "pruned.explain(\"formatted\")\n",
    "show_exchange_nodes(pruned)\n",
    "\n",
    "pruned_plan = pruned._jdf.queryExecution().executedPlan().toString()\n",
    "print(\"contains FileScan:\", \"FileScan\" in pruned_plan)\n",
    "print(\"contains PartitionFilters:\", \"PartitionFilters\" in pruned_plan)\n",
    "\n",
    "pruned_rows = run_and_report(\n",
    "    \"partition-pruning action\",\n",
    "    lambda: pruned.count()\n",
    ")\n",
    "print(\"rows for dt=2024-06-30:\", pruned_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b235035",
   "metadata": {},
   "source": [
    "Plan reading:\n",
    "- Check `FileScan` details for partition filters on `dt`.\n",
    "- This is scan pruning, not shuffle optimization.\n",
    "- `Exchange` is typically unrelated here unless subsequent wide operations are added.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f06e8ef8",
   "metadata": {},
   "source": [
    "### Demo 4 - bucketing (deterministic metadata + plan inspection)\n",
    "\n",
    "This demo creates two bucketed tables with matching bucket key/count and then inspects join plan.\n",
    "We do not assume shuffle elimination blindly; we verify via physical plan and Spark UI in your runtime.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d0756a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.sources.bucketing.enabled\", \"true\")\n",
    "\n",
    "spark.sql(\"DROP TABLE IF EXISTS retail_bucketed_sales\")\n",
    "spark.sql(\"DROP TABLE IF EXISTS retail_bucketed_product\")\n",
    "\n",
    "(\n",
    "    retail\n",
    "    .select(\"product_id\", \"store_id\", \"sale_amount\")\n",
    "    .write\n",
    "    .mode(\"overwrite\")\n",
    "    .bucketBy(8, \"product_id\")\n",
    "    .sortBy(\"product_id\")\n",
    "    .saveAsTable(\"retail_bucketed_sales\")\n",
    ")\n",
    "\n",
    "(\n",
    "    retail\n",
    "    .select(\"product_id\", \"first_category_id\", \"second_category_id\")\n",
    "    .dropDuplicates([\"product_id\", \"first_category_id\", \"second_category_id\"])\n",
    "    .write\n",
    "    .mode(\"overwrite\")\n",
    "    .bucketBy(8, \"product_id\")\n",
    "    .sortBy(\"product_id\")\n",
    "    .saveAsTable(\"retail_bucketed_product\")\n",
    ")\n",
    "\n",
    "print(\"bucketed tables created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61fe402a",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"DESCRIBE EXTENDED retail_bucketed_sales\").show(200, truncate=False)\n",
    "spark.sql(\"DESCRIBE EXTENDED retail_bucketed_product\").show(200, truncate=False)\n",
    "\n",
    "bucket_join = spark.table(\"retail_bucketed_sales\").join(\n",
    "    spark.table(\"retail_bucketed_product\"),\n",
    "    on=\"product_id\",\n",
    "    how=\"inner\"\n",
    ")\n",
    "\n",
    "bucket_join.explain(\"formatted\")\n",
    "show_join_nodes(bucket_join)\n",
    "show_exchange_nodes(bucket_join)\n",
    "\n",
    "bucket_rows = run_and_report(\n",
    "    \"bucket-join action\",\n",
    "    lambda: bucket_join.count()\n",
    ")\n",
    "print(\"bucket join rows:\", bucket_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "689240f8",
   "metadata": {},
   "source": [
    "Plan reading for bucketing:\n",
    "- First validate bucket metadata in `DESCRIBE EXTENDED` outputs.\n",
    "- Then inspect whether join plan still has shuffle `Exchange`.\n",
    "- If `Exchange` is absent for join redistribution, bucket alignment was leveraged.\n",
    "- If `Exchange` remains, planner/runtime did not leverage bucket metadata in this setup.\n",
    "\n",
    "This is exactly why bucketing must be verified in plan/UI, not assumed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3723648a",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "**Shuffle**\n",
    "- `Exchange` marks redistribution and stage boundaries.\n",
    "- Shuffle cost is CPU + disk + network + memory pressure.\n",
    "- Spill is a correctness mechanism with performance cost; reduce it by memory/partition/skew tuning.\n",
    "\n",
    "**Joins**\n",
    "- BHJ: broadcast small side, no shuffle on broadcast path, memory trade-off.\n",
    "- SMJ: shuffle + sort on both sides, scalable default for large joins.\n",
    "- SHJ: shuffle + per-partition hash, avoids sort but can pressure memory.\n",
    "- Skew control is mandatory for stable join latency and spill reduction.\n",
    "\n",
    "**Partitioning and data organization**\n",
    "- `repartition()` is a full shuffle tool; `coalesce()` is a narrow reduce-partitions tool.\n",
    "- Directory partitioning helps scan pruning.\n",
    "- Bucketing can reduce join shuffle only when key/count/sort/metadata/planner conditions align and are verified in plan.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

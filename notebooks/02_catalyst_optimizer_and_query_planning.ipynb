{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31d19433",
   "metadata": {},
   "source": [
    "# 02 - Catalyst Optimizer & Query Planning\n",
    "\n",
    "Focus: understanding **what Spark plans**, **how Catalyst transforms plans**, and **how to read plans for performance**.\n",
    "\n",
    "Quick Catalyst vs Non-Catalyst (Python UDF) performance benchmark: https://rknutalapati.medium.com/understanding-catalyst-optimizer-in-pyspark-catalyst-vs-non-catalyst-functions-48dd1c75d037"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374088ab",
   "metadata": {},
   "source": [
    "## Datasets (local only)\n",
    "\n",
    "Same **two Hugging Face datasets**, already saved locally.\n",
    "\n",
    "- Transaction Categorization (local parquet): `TRANSACTION_PATH`\n",
    "- FreshRetailNet-50K (Retail Sales): `RETAIL_PATH`\n",
    "\n",
    "(We will mostly use the transactions dataset in this notebook; retail is used later in joins / skew notebooks.)\n",
    "\n",
    "Spark UI: http://localhost:404"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00031fd9",
   "metadata": {},
   "source": [
    "## Helper: print each plan stage separately\n",
    "\n",
    "Spark's `df.explain(\"extended\")` prints everything at once.\n",
    "For learning Catalyst, it's useful to inspect each step separately:\n",
    "\n",
    "- **Parsed** (unresolved names, just what you typed)\n",
    "- **Analyzed** (resolved columns / types)\n",
    "- **Optimized** (rule-based rewrites)\n",
    "- **Physical** (operators Spark will run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc40b9a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Below we access Catalyst plans through the underlying JVM DataFrame object (_jdf). \n",
    "# Via queryExecution(), Spark exposes the parsed, analyzed, optimized, and physical plans separately.\n",
    "\n",
    "def get_query_execution(df):\n",
    "    return df._jdf.queryExecution()\n",
    "\n",
    "def show_plan(df, plan_type: str):\n",
    "    qe = get_query_execution(df)\n",
    "    plan_type = plan_type.lower().strip()\n",
    "\n",
    "    if plan_type == \"parsed\":\n",
    "        print(qe.logical().toString())\n",
    "    elif plan_type == \"analyzed\":\n",
    "        print(qe.analyzed().toString())\n",
    "    elif plan_type == \"optimized\":\n",
    "        print(qe.optimizedPlan().toString())\n",
    "    elif plan_type == \"physical\":\n",
    "        print(qe.executedPlan().toString())\n",
    "    else:\n",
    "        raise ValueError(\"plan_type must be: parsed | analyzed | optimized | physical\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6179843b",
   "metadata": {},
   "source": [
    "## Load dataset (transactions)\n",
    "\n",
    "Set your local parquet path and load it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1bf2eb87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- transaction_description: string (nullable = true)\n",
      " |-- category: string (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- currency: string (nullable = true)\n",
      "\n",
      "rows: 4501043\n",
      "partitions: 4\n"
     ]
    }
   ],
   "source": [
    "# Load parquet\n",
    "txn = spark.read.parquet(r\"C:\\code\\spark-tuning-handbook\\data\\transaction_cat.parquet\")\n",
    "txn.printSchema()\n",
    "print(\"rows:\", txn.count())\n",
    "print(\"partitions:\", txn.rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd1879b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## What is Catalyst\n",
    "\n",
    "Catalyst is Spark SQL's optimizer and planner.\n",
    "\n",
    "You write DataFrame / SQL code -> Spark builds a **logical plan** -> Catalyst rewrites it -> Spark produces a **physical plan** (operators like scans, exchanges, hashes, sorts).\n",
    "\n",
    "For performance you care about:\n",
    "- where **filters** happen (pushed into scan or not)\n",
    "- how many **columns** are read (pruning)\n",
    "- where **shuffles** appear (Exchange)\n",
    "- whether Spark uses **codegen** (WholeStageCodegen)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e9d370",
   "metadata": {},
   "source": [
    "The planning pipeline\n",
    "\n",
    "1) **Parsed** plan: what you typed (names are not validated yet)\n",
    "2) **Analyzed** plan: columns and types are resolved\n",
    "3) **Optimized** plan: Catalyst rewrites (push filters/projects, fold constants, simplify expressions)\n",
    "4) **Physical** plan: concrete execution operators\n",
    "\n",
    "Next: build one query and inspect each stage **in separate cells**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "243df9f9",
   "metadata": {},
   "source": [
    "## One query, four plans\n",
    "\n",
    "Query:\n",
    "- filter by country\n",
    "- join currency_lookup\n",
    "- compute a derived column\n",
    "- select a few columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "832331ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[country: string, currency_name: string, desc_len: int]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# currecny lookup table\n",
    "currency_lookup = spark.createDataFrame([\n",
    "    (\"USD\", \"US Dollar\"),\n",
    "    (\"GBP\", \"British Pound\"),\n",
    "    (\"EUR\", \"Euro\")\n",
    "], [\"currency_code\", \"currency_name\"])\n",
    "\n",
    "query = (\n",
    "    txn\n",
    "    .filter(F.col(\"country\") == \"USA\")\n",
    "    .join(currency_lookup, txn[\"currency\"] == currency_lookup[\"currency_code\"])\n",
    "    .withColumn(\"desc_len\", F.length(\"transaction_description\"))\n",
    "    .select(\"country\", \"currency_name\", \"desc_len\")\n",
    ")\n",
    "\n",
    "# no actions, only transformations description (Lazy Evaluation)\n",
    "query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "037970a4-6891-4169-a172-6fb714e1c8f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "Join Inner, (currency#15 = currency_code#24)\n",
      ":- Relation [transaction_description#12,category#13,country#14,currency#15] parquet\n",
      "+- ResolvedHint (strategy=broadcast)\n",
      "   +- LogicalRDD [currency_code#24, currency_name#25], false\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "transaction_description: string, category: string, country: string, currency: string, currency_code: string, currency_name: string\n",
      "Join Inner, (currency#15 = currency_code#24)\n",
      ":- Relation [transaction_description#12,category#13,country#14,currency#15] parquet\n",
      "+- ResolvedHint (strategy=broadcast)\n",
      "   +- LogicalRDD [currency_code#24, currency_name#25], false\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Join Inner, (currency#15 = currency_code#24), rightHint=(strategy=broadcast)\n",
      ":- Filter isnotnull(currency#15)\n",
      ":  +- Relation [transaction_description#12,category#13,country#14,currency#15] parquet\n",
      "+- Filter isnotnull(currency_code#24)\n",
      "   +- LogicalRDD [currency_code#24, currency_name#25], false\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- BroadcastHashJoin [currency#15], [currency_code#24], Inner, BuildRight, false\n",
      "   :- Filter isnotnull(currency#15)\n",
      "   :  +- FileScan parquet [transaction_description#12,category#13,country#14,currency#15] Batched: true, DataFilters: [isnotnull(currency#15)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/C:/code/spark-tuning-handbook/data/transaction_cat.parquet], PartitionFilters: [], PushedFilters: [IsNotNull(currency)], ReadSchema: struct<transaction_description:string,category:string,country:string,currency:string>\n",
      "   +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, false]),false), [plan_id=235]\n",
      "      +- Filter isnotnull(currency_code#24)\n",
      "         +- Scan ExistingRDD[currency_code#24,currency_name#25]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# In production, you would typically use query.explain(extended=True).\n",
    "# We use show_plan() here to isolate each stage for educational clarity.\n",
    "# Standard explain() outputs a dense block that can be harder to parse visually.\n",
    "\n",
    "# query.explain() # short Phisical Plan\n",
    "# query.explain(\"formatted\")  # detailed Phisical Plan\n",
    "query.explain(extended=True) # complete plan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc7f58c",
   "metadata": {},
   "source": [
    "### Parsed logical plan (unresolved)\n",
    "**Logical Tree**: Spark organizes the query as a hierarchy. It reads from the bottom up: it starts at the Relation (source), applies a Filter, and finally performs a Project (selection).\n",
    "\n",
    "**Tree Connectors** +- and :- show the parent-child relationship between operators. A +- indicates the last child of a node, while :- would indicate a preceding child (used when a node has multiple inputs, like a Join).\n",
    "\n",
    "Notice:\n",
    "- **Unresolved parts**: New columns or selections often appear with a tick (e.g., 'country) and no ID yet.\n",
    "- **Inherited IDs**: Because we started from an existing DataFrame (txn), existing columns already have their internal IDs (like #123).\n",
    "- **Logic check**: Spark has parsed Python code into a logical tree but hasn't verified if the new operations (like length()) are valid for those types yet.\n",
    "- **Batch vs. Streaming**: The false flag in the LogicalRDD node confirms the data is a static Batch dataset rather than a continuous Stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "60aad142",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Project ['country, 'currency_name, 'desc_len]\n",
      "+- Project [transaction_description#12, category#13, country#14, currency#15, currency_code#24, currency_name#25, length(transaction_description#12) AS desc_len#27]\n",
      "   +- Join Inner, (currency#15 = currency_code#24)\n",
      "      :- Filter (country#14 = USA)\n",
      "      :  +- Relation [transaction_description#12,category#13,country#14,currency#15] parquet\n",
      "      +- LogicalRDD [currency_code#24, currency_name#25], false\n",
      "\n"
     ]
    }
   ],
   "source": [
    "show_plan(query, 'parsed')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "066217f9-b683-471d-a1de-40c5dca8ec2d",
   "metadata": {},
   "source": [
    "### Logical Tree Structure\n",
    "```text\n",
    "Project (Root / Output)\n",
    "  └── Project (Calculation Node)\n",
    "        └── Join (Binary Node)\n",
    "              /            \\\n",
    "     Left Child (:-)    Right Child (+-)\n",
    "            |               |\n",
    "       Filter Node      Leaf Node (LogicalRDD)\n",
    "            |\n",
    "    Leaf Node (Relation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb2ad5cf",
   "metadata": {},
   "source": [
    "### Analyzed logical plan (resolved)\n",
    "\n",
    "In our local environment, Saprk uses an In-memory Catalog to resolve the schema. It retrieves the schema either from file metadata (Parquet, Avro, ORC) or by scanning the data AKA InferSchema (CSV, JSON), then caches this structure in the current SparkSession's memory.\n",
    "\n",
    "Notice:\n",
    "- **Validation & Errors**: This stage triggers an AnalysisException if Spark fails to map column or table names to the actual schema, ensuring all references are validated before optimization.\n",
    "- **Attribute Resolution**: Each column is assigned a unique Expression ID, allowing Spark to track attributes even if column names overlap across tables.\n",
    "- **Type Validation**: Spark verifies the schema and confirms data types. It ensures functions like length() are only applied to compatible types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eca1c366",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project [country#14, currency_name#25, desc_len#27]\n",
      "+- Project [transaction_description#12, category#13, country#14, currency#15, currency_code#24, currency_name#25, length(transaction_description#12) AS desc_len#27]\n",
      "   +- Join Inner, (currency#15 = currency_code#24)\n",
      "      :- Filter (country#14 = USA)\n",
      "      :  +- Relation [transaction_description#12,category#13,country#14,currency#15] parquet\n",
      "      +- LogicalRDD [currency_code#24, currency_name#25], false\n",
      "\n"
     ]
    }
   ],
   "source": [
    "show_plan(query, 'analyzed')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d2836a",
   "metadata": {},
   "source": [
    "### Optimized logical plan (Catalyst rewrites)\n",
    "\n",
    "This stage is the core of **Rule-Based Optimization (RBO)**. Spark uses the Catalyst optimizer to apply a series (~50) of deterministic rules that transform the logical tree into a more efficient version **without data statistics**.\n",
    "\n",
    "**How it works**: Catalyst groups optimization rules into Batches. Each batch is executed repeatedly until it reaches a Fixed Point (the plan stops changing), ensuring that even simple rules can have a large global impact.\n",
    "\n",
    "Notice:\n",
    "- **Predicate Pushdown**: Moves filters (like isnotnull or EqualTo) as close to the data source as possible. This minimizes the number of rows that participate in expensive operations like Joins.\n",
    "- **Column Pruning**: Detects which columns are actually needed for the final result and removes all others early in the plan to reduce memory and CPU overhead.\n",
    "- **Constant Folding**: Pre-calculates expressions involving constants (e.g., 100 * 0.5 becomes 50.0) during the optimization phase so they aren't calculated for every row during execution.\n",
    "- **Inferring Null Filters**: As seen in the plan, Spark automatically injects Filter isnotnull(...) on join keys. This \"null-safe\" optimization prevents the engine from attempting to match null values, which would never satisfy an inner join."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "64fe64d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project [country#14, currency_name#25, length(transaction_description#12) AS desc_len#27]\n",
      "+- Join Inner, (currency#15 = currency_code#24)\n",
      "   :- Project [transaction_description#12, country#14, currency#15]\n",
      "   :  +- Filter ((isnotnull(country#14) AND (country#14 = USA)) AND isnotnull(currency#15))\n",
      "   :     +- Relation [transaction_description#12,category#13,country#14,currency#15] parquet\n",
      "   +- Filter isnotnull(currency_code#24)\n",
      "      +- LogicalRDD [currency_code#24, currency_name#25], false\n",
      "\n"
     ]
    }
   ],
   "source": [
    "show_plan(query, 'optimized')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7717149a",
   "metadata": {},
   "source": [
    "### Physical plan (execution operators)\n",
    "\n",
    "This stage transitions from \"what to do\" (logic) to **\"how to do it\"** (execution). While RBO worked with logical rules, the Physical Planning phase uses Cost-Based Optimization (CBO) to select the most efficient physical algorithms.\n",
    "\n",
    "**Key Concepts:**\n",
    "- **Algorithm Selection**: Spark evaluates different physical operators for the same logic. For example, it chooses BroadcastHashJoin (fastest, memory-intensive) or SortMergeJoin (scalable, shuffle-intensive) based on data size.\n",
    "- **Cost-Based Optimizer (CBO)**: Catalyst calculates the \"cost\" (CPU, I/O, Network) of multiple physical plans and picks the cheapest one.\n",
    "- **Statistics Matter**: CBO relies on data stats (row count, table size, histograms). Without running ANALYZE TABLE, Spark makes decisions based on rough estimates, which can lead to suboptimal plans. However, no statistics are often better than stale statistics. Outdated stats can mislead the optimizer into picking a disastrous plan.\n",
    "\n",
    "Notice:\n",
    "- **SortMergeJoin**: In this plan, Spark selected SortMergeJoin because the datasets are treated as potentially large.\n",
    "- **Exchange (Shuffle)**: You can see Exchange hashpartitioning, which indicates a Shuffle operation-Spark is re-distributing data across the cluster to align join keys.\n",
    "- **WholeStageCodegen**: The asterisks (*) next to operators (e.g., *Project) show that Spark has collapsed these steps into a single highly-optimized Java function to improve performance. Will be displayed after an action with AdaptiveSparkPlan isFinalPlan=true."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "17ee6780",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Project [country#14, currency_name#25, length(transaction_description#12) AS desc_len#27]\n",
      "   +- SortMergeJoin [currency#15], [currency_code#24], Inner\n",
      "      :- Sort [currency#15 ASC NULLS FIRST], false, 0\n",
      "      :  +- Exchange hashpartitioning(currency#15, 200), ENSURE_REQUIREMENTS, [plan_id=113]\n",
      "      :     +- Filter ((isnotnull(country#14) AND (country#14 = USA)) AND isnotnull(currency#15))\n",
      "      :        +- FileScan parquet [transaction_description#12,country#14,currency#15] Batched: true, DataFilters: [isnotnull(country#14), (country#14 = USA), isnotnull(currency#15)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/C:/code/spark-tuning-handbook/data/transaction_cat.parquet], PartitionFilters: [], PushedFilters: [IsNotNull(country), EqualTo(country,USA), IsNotNull(currency)], ReadSchema: struct<transaction_description:string,country:string,currency:string>\n",
      "      +- Sort [currency_code#24 ASC NULLS FIRST], false, 0\n",
      "         +- Exchange hashpartitioning(currency_code#24, 200), ENSURE_REQUIREMENTS, [plan_id=114]\n",
      "            +- Filter isnotnull(currency_code#24)\n",
      "               +- Scan ExistingRDD[currency_code#24,currency_name#25]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "show_plan(query, 'physical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "15876249-8e9a-4e28-8bbf-88663c00e064",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- BroadcastHashJoin [currency#15], [currency_code#24], Inner, BuildRight, false\n",
      "   :- Filter isnotnull(currency#15)\n",
      "   :  +- FileScan parquet [transaction_description#12,category#13,country#14,currency#15] Batched: true, DataFilters: [isnotnull(currency#15)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/C:/code/spark-tuning-handbook/data/transaction_cat.parquet], PartitionFilters: [], PushedFilters: [IsNotNull(currency)], ReadSchema: struct<transaction_description:string,category:string,country:string,currency:string>\n",
      "   +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, false]),false), [plan_id=235]\n",
      "      +- Filter isnotnull(currency_code#24)\n",
      "         +- Scan ExistingRDD[currency_code#24,currency_name#25]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Generating statistics locally without a persistent metastore is a headache.\n",
    "# Another way to \"help\" the Physical Plan is by using Hints\n",
    "\n",
    "from pyspark.sql.functions import broadcast\n",
    "\n",
    "query = txn.join(broadcast(currency_lookup), txn.currency == currency_lookup.currency_code)\n",
    "show_plan(query, 'physical')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f305bb51",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Topic 8 - How to read plans fast\n",
    "\n",
    "Order:\n",
    "1) Scan (FileScan / pushed filters / read schema)\n",
    "2) Exchange (shuffles)\n",
    "3) Aggregations (partial + final)\n",
    "4) Sort\n",
    "5) WholeStageCodegen\n",
    "\n",
    "Spot the expensive ones: **Exchange, Sort, huge scans**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
